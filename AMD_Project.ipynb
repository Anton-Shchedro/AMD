{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "XLBhQehxKDC_",
        "outputId": "c3bb6600-d1f9-4024-8c0f-3963795dd57f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c180dc66-1b87-445e-a89f-f5256d5c9107\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c180dc66-1b87-445e-a89f-f5256d5c9107\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()  # upload kaggle.json with API token\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3HNl6VEN2OX",
        "outputId": "b6383742-851c-4212-e86d-5960cb48f5e9",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true,
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip to /content\n",
            "100% 13.9G/13.9G [02:53<00:00, 34.5MB/s]\n",
            "100% 13.9G/13.9G [02:53<00:00, 86.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg9egW5QQAXk",
        "outputId": "8aa164f7-3fb1-495d-96c0-5fdfcf2d303f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip\n",
            "  inflating: twits/0819_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0820_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0821_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0822_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0823_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0824_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0825_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0826_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0827_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0828_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0829_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0830_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0831_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0901_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0902_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0903_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0904_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0905_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0906_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0907_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0908_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0909_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0910_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0911_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0912_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0913_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0914_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0915_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0916_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0917_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0918_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0919_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0920_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0921_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0922_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0923_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0924_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0925_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0926_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0927_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0928_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0929_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/0930_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1001_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1002_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1003_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1004_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1005_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1006_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1007_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1008_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1009_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1010_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1011_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1012_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1013_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1014_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1015_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1016_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1017_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1018_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1019_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1020_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1021_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1022_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1023_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1024_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1025_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1026_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1027_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1028_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1029_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1030_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1031_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1101_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1102_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1103_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1104_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1105_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1106_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1107_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1108_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1109_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1110_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1111_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1112_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1113_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1114_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1115_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1116_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1117_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1118_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1119_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1120_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1121_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1122_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1123_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1124_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1125_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1126_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1127_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1128_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1129_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1130_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1201_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1202_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1203_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1204_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1205_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1206_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1207_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1208_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1209_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1210_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1211_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1212_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1213_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1214_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1215_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1216_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1217_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1218_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1219_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1220_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1221_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1222_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1223_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1224_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1225_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1226_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1227_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1228_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1229_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1230_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/1231_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230101_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230102_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230103_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230104_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230105_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230106_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230107_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230108_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230109_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230110_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230111_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230112_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230113_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230114_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230115_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230116_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230117_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230118_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230119_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230120_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230121_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230122_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230123_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230124_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230125_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230126_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230127_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230128_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230129_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230130_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230131_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230201_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230202_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230203_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230204_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230205_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230206_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230207_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230208_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230209_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230210_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230211_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230212_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/20230213_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0401_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0402_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0403_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0404_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0405_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0406_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0407_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0408_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0409_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0410_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0411_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0412_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0413_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0414_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0415_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0416_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0417_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0418_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0419_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0420_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0421_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0422_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0423_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0424_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0425_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0426_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0427_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0428_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0429_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0430_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0501_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0502_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0503_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0504_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0505_to_0507_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0508_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0509_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0510_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0511_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0512_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0513_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0514_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0515_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0516_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0517_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0518_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0519_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0520_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0521_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0522_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0523_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0524_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0525_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0526_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0527_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0528_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0529_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0530_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0531_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0601_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0602_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0603_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0604_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0605_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0606_to_08_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0609_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0610_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0611_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0612_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0613_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0614_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0615_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0616_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0617_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0618_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0619_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0620_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0621_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0622_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0623_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0624_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0625_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0626_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0627_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0628_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0629_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0630_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0701_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0702_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0703_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0704_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0705_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0706_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0707_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0708_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0709_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0710_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0711_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0712_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0713_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0714_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0715_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0716_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0717_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0718_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0719_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0720_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0721_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0722_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0723_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0724_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0725_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0726_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0727_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0728_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0729_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0730_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0731_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0801_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0802_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0803_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0804_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0805_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0806_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0807_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0808_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0809_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0810_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0811_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0812_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0813_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0814_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0815_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0816_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0817_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/0818_UkraineCombinedTweetsDeduped.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped20220227-131611.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB27.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB28_part1.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_FEB28_part2.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR01.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR02.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR03.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR04.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR05.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR06.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR07.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR08.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR09.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR10.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR11.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR12.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR13.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR14.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR15.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR16.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR17.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR18.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR19.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR20.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR21.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR22.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR23.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR24.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR25.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR26.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR27_to_28.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR29.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR30.csv.gzip  \n",
            "  inflating: twits/UkraineWar/UkraineWar/UkraineCombinedTweetsDeduped_MAR31.csv.gzip  \n"
          ]
        }
      ],
      "source": [
        "! mkdir twits\n",
        "! unzip './ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip' -d twits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0I_I67MRsHU",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "!rm /content/ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCvo92mTMFew",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Project 1: Finding similar items\n",
        "\n",
        "The task is to implement from scratch a detector of pairs of similar tweets, considering the text column of the dataset and selecting tweets written in a given language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmbGVmEp82fp",
        "outputId": "60300506-82a1-4daf-fc4d-8f99c470ef9f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=a4e16c56d02a44f8b07e803a5aeff9180dc56d3245b61ef4477057fa7c29397c\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lESGst1xDWxR"
      },
      "outputs": [],
      "source": [
        "conf = pyspark.SparkConf().set('spark.ui.port', '4050')\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = pyspark.sql.SparkSession.builder.master('local[*]').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "MW_38hjNDWxR",
        "outputId": "faf4c9c5-b074-4c98-c496-c36096cc01ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f6ce4e123d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6deb2462c162:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IK5yiluJCFgC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEtYH_KZDWxS"
      },
      "source": [
        "======================================================\n",
        "\n",
        "**Use only one of two following cells of code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hlfh95wDWxS"
      },
      "source": [
        "Read and load rdd and df of one particular file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEztQ7W6eNAY",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "filename = './twits/0820_UkraineCombinedTweetsDeduped.csv.gzip'\n",
        "small_twits_df = pd.read_csv(filename, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "en_small_twits_df = small_twits_df[small_twits_df['language'] == 'en']\n",
        "en_small_twits_df = en_small_twits_df.reset_index(drop=True)\n",
        "data = en_small_twits_df[['tweetid','text']]\n",
        "#spark = pyspark.sql.SparkSession.builder.appName('pd_to_rdd').getOrCreate()\n",
        "rdd = spark.createDataFrame(data).rdd\n",
        "#rdd.take(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhSZh2LpDWxT"
      },
      "source": [
        "Load df from directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PJRibnJPhYU",
        "outputId": "90e7c7a8-5390-460d-ca5b-cf0e1a2fef50",
        "pycharm": {
          "name": "#%%\n"
        },
        "scrolled": true,
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<DirEntry '0819_UkraineCombinedTweetsDeduped.csv.gzip'>\n",
            "<DirEntry '0820_UkraineCombinedTweetsDeduped.csv.gzip'>\n",
            "<DirEntry '0821_UkraineCombinedTweetsDeduped.csv.gzip'>\n",
            "<DirEntry '0822_UkraineCombinedTweetsDeduped.csv.gzip'>\n",
            "<DirEntry '0823_UkraineCombinedTweetsDeduped.csv.gzip'>\n",
            "<DirEntry '0824_UkraineCombinedTweetsDeduped.csv.gzip'>\n",
            "<DirEntry '0825_UkraineCombinedTweetsDeduped.csv.gzip'>\n",
            "<DirEntry '0826_UkraineCombinedTweetsDeduped.csv.gzip'>\n",
            "<DirEntry '0827_UkraineCombinedTweetsDeduped.csv.gzip'>\n",
            "<DirEntry '0828_UkraineCombinedTweetsDeduped.csv.gzip'>\n",
            "<DirEntry '0829_UkraineCombinedTweetsDeduped.csv.gzip'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "UnionRDD[98] at union at <unknown>:0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_list = '' # list of loaded files\n",
        "rdd = None\n",
        "for file in os.scandir('./twits/'):\n",
        "    if not os.path.isdir(file):\n",
        "        print(file)\n",
        "        file_list = file_list + file.path + '/n'\n",
        "        twits_df = pd.read_csv(file, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "        twits_df = twits_df[twits_df['language'] == 'en']\n",
        "        twits_df = twits_df.reset_index(drop=True)\n",
        "        data = twits_df[['tweetid','text']]\n",
        "        if rdd is None:\n",
        "            #spark = pyspark.sql.SparkSession.builder.appName('pd_to_rdd').getOrCreate()\n",
        "            rdd = spark.createDataFrame(data).rdd\n",
        "        else:\n",
        "            rdd1 = spark.createDataFrame(data).rdd\n",
        "            rdd = rdd.union(rdd1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLKgJZmh_6PD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Shingling\n",
        "\n",
        "2 possible methods:\n",
        "\n",
        "1.   Token as char\n",
        "2.   Token as word\n",
        "\n",
        "I try Token as word here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj21DuUYJyH7",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Tokenize:\n",
        "\n",
        "1.   remove emoji\n",
        "2.   split in words\n",
        "3.   remove stopwords\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcPoaTJ9BkaX",
        "outputId": "6617135e-1518-49c0-b8d9-4a765d9b47e3",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=2a83be49166db1f9d15b2d5b8b5107a3905879eb9f36b0cbb45ab08821b26e3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji\n",
        "from emoji import EMOJI_DATA, replace_emoji # EMOJI_DATA is a list of emojies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSb4qfu9KL2n",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "I use NLTK only to take list of stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMZx6I1WGQbE",
        "outputId": "a528954a-6591-4701-907d-e0a90f810a83",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_en = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4M_5H4bCa66",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Additionaly need to add URL removal from text (solution from https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python by Lee Martin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnBO8aGY_4Tx",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def remove_urls(vTEXT):\n",
        "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
        "    return(vTEXT)\n",
        "\n",
        "def tokenize(text):\n",
        "  s = replace_emoji(text, replace='')\n",
        "  s = remove_urls(s)\n",
        "  if s != '':\n",
        "    #words = re.split(r'[^\\w'+removelist+']', s.lower())         # uncoment this if need to distinguish between hashtag and not hashtag words\n",
        "    words = re.split(r'\\W', s.lower())\n",
        "    return [w for w in words if not w in stop_en and w != '']\n",
        "\n",
        "#print('The World Health Organization (#WHO) says that two existing treatments dramatically reduced deaths from #Ebola and should be given to people of all ages suffering from the often-fatal hemorrhagic disease.\\nhttps://t.co/iHF0FPULBU')\n",
        "#print(remove_urls(en_small_twits_df.iloc[0]['text']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myx5qNfGKNae",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Create n-grams of words for each line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhQjPyCWDHGa",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def n_gram(tokens, n):\n",
        "  if len(tokens) < n:\n",
        "    l = [''] * (n-len(tokens))\n",
        "    return tokens + l\n",
        "  l = []\n",
        "  for i in range(len(tokens)-n+1):\n",
        "    l.append(' '.join(tokens[i:i+n]))\n",
        "  return l\n",
        "\n",
        "#n_gram(['a','b','c','d'],2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Gx7tzLDWxV"
      },
      "source": [
        "**! List of shingles may contain similar shingles that can be in merged together, but i will not do it. Merge can be done only in case of k=2. Example of this shingles: 'russia ukraine' and 'ukraine russia'**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPFQzvcYNYqa",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Min-hashing and LSH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdJUry5-HJaL",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Create matrix of min-hash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aibgakW24cNN",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Hash function: take list of shingles of one twitt, transform it to a int value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPHt4tTquXyI",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def string_hash(text, p, n):\n",
        "  h = np.uint32(5381) #Bernstein's hash function djb2\n",
        "  for i in range(len(text)):\n",
        "    h = ((h*33) + ord(text[i])) % p\n",
        "  return h % n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luvZ1CiqHGKC",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def generate_primes(n, m):\n",
        "  #generates list of prime numbers larger than m\n",
        "  #it (if i made all correct) have to return same list for same input\n",
        "  primes = []\n",
        "  num = m + 1 if m % 2 == 0 else m + 2 # Take first odd number grater than m\n",
        "  while len(primes) < n:\n",
        "    for i in range(3, int(num ** 0.5) + 1, 2): # no need to check if num is even (is always odd).\n",
        "      # check if num is prime\n",
        "      if num % i == 0:\n",
        "        break\n",
        "    else: #if num is prime (for is not breaked)\n",
        "      primes.append(num)\n",
        "    num += 2 # take next odd number to check\n",
        "  return primes\n",
        "\n",
        "#generate_primes(100, 2**32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs8OeDGQBcSF",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Signature of twitt: For each hash function calcolate hash value of each shingle, take minimum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU5PDtMTDWxW"
      },
      "outputs": [],
      "source": [
        "def signature(text, n, m, primes): \n",
        "  #text - list of shingles in one record, n - num of hash functins, m - max value of hash\n",
        "  # for example i use n = 100, m = 2**32\n",
        "\n",
        "  signature = [np.inf]*n\n",
        "  for i, p in enumerate(primes):\n",
        "    for shingle in text:\n",
        "      h = string_hash(shingle, p, m)\n",
        "      if h < signature[i]:\n",
        "        signature[i] = h\n",
        "  return signature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i22MOM1oAnin",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**Optional**\n",
        "\n",
        "Jaccard similarity funtion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWFqoW_4qQZS",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#jaccard similarity\n",
        "def jaccard(a,b):\n",
        "  intersection = len(list(set(a).intersection(b)))\n",
        "  union = (len(set(a)) + len(set(b))) - intersection\n",
        "  return float(intersection) / union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8k-UB6ADWxX"
      },
      "source": [
        "**LSH**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSi5XWshB7H-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "we need to divide signature made of 100 minhash values in b bands of r rows (enough to specify b that 100%b = 0, for example b = 20) and put it in k baskets (b*k total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciM-VFY0qJ2_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "For 200 minhash values keep b = 20, r = 10, s = 0.75\n",
        "\n",
        "For 100 minhash values keep b = 20, r = 5, s = 0.54\n",
        "\n",
        "$s = (1/b)^{1/r}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRO8eHGt8I9h",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def lsh(text, b, k):\n",
        "  #return list of baskets indexes that this recort corrensponds to.\n",
        "  l = [None]*b\n",
        "  r = round(len(text)/b)\n",
        "  for i in range(b):\n",
        "    try:\n",
        "      # take part of signeture of batch, sum it and mod k to find a bucet index from k possible backets\n",
        "      # add i*k to separate backets of different batches.\n",
        "      l[i] = (np.sum(text[r*i:r*(i+1)]) % k) + i*k\n",
        "    except:\n",
        "      print(i)\n",
        "      raise Exception('LSH Error')\n",
        "  return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCSLi_1NDWxX"
      },
      "outputs": [],
      "source": [
        "#List of parameters that were used to calculate LSH\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "\n",
        "primes = generate_primes(N_HASH, MAX_HASH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBCvX3oRISc7",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoqE-9I0DWxX"
      },
      "source": [
        "Prepare LSH from rdd of twitts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aEx0sjSJ5px",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gIJBS8mDWxY"
      },
      "source": [
        "**Optional**\n",
        "\n",
        "collect all LSH buckets in cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwHZLqHdrCKc",
        "outputId": "261be93d-b5a3-457b-f903-d1b24694a7ea",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(408, [1560778639343587328, 1560795505000595456, 1560807732059545601, 1560810931789053952, 1560817894732300289, 1560882204926160897, 1560884969525886976, 1560891814617395200, 1560898735172878337, 1560914368728268801, 1560925610037444609, 1560927621441064960, 1560931108598030341, 1560932889793019904, 1560940622978207744, 1560954634134605825, 1560981826680012803, 1560988496327761922, 1560997234707091470, 1561001375785443328, 1561007785369899014, 1561027237956182018, 1561027596829212673, 1561041096037261313, 1561069833109872647, 1561096187457183745, 1561098894519459840, 1561112622359003137, 1561114220934176774, 1561114850838958081, 1561115191898648576])\n",
            "458.3296239376068\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "collect_lsh_s = lsh_s.collect()\n",
        "end_time = time.time() - start_time\n",
        "#print(collect_lsh_s[0])\n",
        "print(end_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB2rmx--DWxY",
        "outputId": "61d8e967-050e-4ea2-961f-3c695d2d5017"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "168848"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import sys\n",
        "sys.getsizeof(collect_lsh_s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-25OecT-DWxZ"
      },
      "source": [
        "## Find similar items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU9sYsNEDWxZ"
      },
      "source": [
        "Count number of occurence of twitts in buckets that contain searched twitt\n",
        "\n",
        "Filter results that have at least *s* occurences (from 1 to BANDS *check for costans in the beginning of notebook*)\n",
        "\n",
        "If searched twitt is also in a LSH, result will also contain that twitt id with BANDS value as similatiry."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_similar(tweet_id, tweet_text, lsh_s, similarity):\n",
        "    if similarity > BANDS:\n",
        "        print(\"Maximum similarity cannot exeed number of bashes: \" + str(BANDS))\n",
        "        return None\n",
        "    search_signature = (tweet_id,lsh(signature(n_gram(tokenize(tweet_text),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "    answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=similarity).collect()\n",
        "    return (tweet_id, answer)"
      ],
      "metadata": {
        "id": "KiJnJjbPBJH5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_record = en_small_twits_df[['tweetid','text']].sample(1).iloc[0]\n",
        "search_similar(search_record['tweetid'], search_record['text'], lsh_s, 3)"
      ],
      "metadata": {
        "id": "hrSF11qiBNfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPKDajOeDWxa",
        "outputId": "e1489b36-af04-4432-e296-52cb7fd5cf22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tweetid                                  1561010740189974529\n",
            "text       #Leaders #AI #transformation\\n🎯#Fox #Business:...\n",
            "Name: 11888, dtype: object\n",
            "[(1560942577016258561, 20), (1561010740189974529, 20)]\n"
          ]
        }
      ],
      "source": [
        "# search by twitt id\n",
        "search_record = en_small_twits_df[en_small_twits_df['tweetid'] == 1561010740189974529].iloc[0][['tweetid','text']]\n",
        "print(search_record)\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=10).collect()\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as search_similar function, but print time of it's parts:"
      ],
      "metadata": {
        "id": "dokZ4NtfBfJ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh-lb132DWxZ",
        "outputId": "8f41cf09-93fa-4225-f25d-1e49d59c4c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tweetid                       1561001974484848641\n",
            "text       #Putin likes little boys #Pedoputin 👬🤮\n",
            "Name: 11257, dtype: object\n",
            "signature count time: 0.006654024124145508\n",
            "search time: 0.44415783882141113\n",
            "[(1560970279572176896, 20), (1560970918188519426, 20), (1561001346211389440, 20), (1561001375584161800, 20), (1561001449408143364, 20), (1561001709769572352, 20), (1561001899930882048, 20), (1560971086354911235, 20), (1561001313340891137, 20), (1561001655684198403, 20), (1561001737292754945, 9), (1561001796516155393, 20), (1561001974484848641, 20)]\n"
          ]
        }
      ],
      "source": [
        "# Random twitt from df\n",
        "search_record = en_small_twits_df[['tweetid','text']].sample(1).iloc[0]\n",
        "print(search_record)\n",
        "import time\n",
        "start_time = time.time()\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "print(\"signature count time: \" + str(time.time() - start_time))\n",
        "start_time = time.time()\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).collect()\n",
        "print(\"search time: \" + str(time.time() - start_time))\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-8VUN5IDWxa"
      },
      "source": [
        "## Optional: Calculate and save LSH of dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI4crqSP8Jz5",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Create LSH textfile\n",
        "\n",
        "Made 1 lsh file for 9 twitter csv files.\n",
        "\n",
        "(Wanted 10, but because i made an error in code 9. This is not critical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTGFakD58B6S",
        "outputId": "8cf13762-0210-494b-de1d-9c0c26ed9cd2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done: 9 / 178\n"
          ]
        }
      ],
      "source": [
        "total_files = 0\n",
        "for file in os.scandir('./twits/'):\n",
        "  if not os.path.isdir(file):\n",
        "    total_files+=1\n",
        "\n",
        "i = 0\n",
        "j = 0\n",
        "lsh_file_list = ''\n",
        "\n",
        "for file in os.scandir('./twits/'):\n",
        "    if not os.path.isdir(file):\n",
        "        twits_df = pd.read_csv(file, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "        twits_df = twits_df[twits_df['language'] == 'en']\n",
        "        twits_df = twits_df.reset_index(drop=True)\n",
        "        data = twits_df[['tweetid','text']]\n",
        "        if i == 0:\n",
        "            rdd = spark.createDataFrame(data).rdd\n",
        "        else:\n",
        "            rdd1 = spark.createDataFrame(data).rdd\n",
        "            rdd = rdd.union(rdd1)\n",
        "        lsh_file_list = lsh_file_list + file.path + '\\n'\n",
        "        i+=1\n",
        "        if i == 9:\n",
        "            shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),2)))\n",
        "            lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "            collect_lsh_s = lsh_s.collect()\n",
        "            filename = 'lsh_s_' + str(j) + '.txt'\n",
        "            with open(filename, 'w') as f:\n",
        "                for line in collect_lsh_s:\n",
        "                    f.write(f\"{line}\\n\")\n",
        "            filename = 'lsh_filelist_' + str(j) + '.txt'\n",
        "            with open(filename, 'w') as f:\n",
        "                f.write(lsh_file_list)\n",
        "            j+=1\n",
        "            i = 0\n",
        "            lsh_file_list = ''\n",
        "            print(\"Done: \" + str(j*9) + \" / \" + str(total_files))\n",
        "if i != 0:\n",
        "    shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),2)))\n",
        "    lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "    collect_lsh_s = lsh_s.collect()\n",
        "    filename = 'lsh_s_' + str(j) + '.txt'\n",
        "    with open(filename, 'w') as f:\n",
        "        for line in collect_lsh_s:\n",
        "            f.write(f\"{line}\\n\")\n",
        "    filename = 'lsh_filelist_' + str(j) + '.txt'\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(lsh_file_list)\n",
        "    j+=1\n",
        "    print(\"Done: \" + str(j*9) + \" / \" + str(total_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP1orF8iDWxb"
      },
      "source": [
        "Create list of twitts ID and file that contain it \n",
        "\n",
        "(Can be used if all twitter files are not loaded into a single rdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-kbd5y-DWxb",
        "outputId": "a1b332ef-5385-42e9-b110-69b0a88e7c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done: 9 / 129\n",
            "Done: 18 / 129\n",
            "Done: 27 / 129\n",
            "Done: 36 / 129\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Anton\\AppData\\Local\\Temp\\ipykernel_27468\\3213938757.py:11: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  twits_df = pd.read_csv(file, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done: 45 / 129\n",
            "Done: 54 / 129\n",
            "Done: 63 / 129\n",
            "Done: 72 / 129\n",
            "Done: 81 / 129\n",
            "Done: 90 / 129\n",
            "Done: 99 / 129\n",
            "Done: 108 / 129\n",
            "Done: 117 / 129\n",
            "Done: 126 / 129\n",
            "Done: 129 / 129\n"
          ]
        }
      ],
      "source": [
        "total_files = 0\n",
        "for file in os.scandir('./twits/'):\n",
        "  if not os.path.isdir(file):\n",
        "    total_files+=1\n",
        "\n",
        "i = 0\n",
        "j = 0\n",
        "\n",
        "for file in os.scandir('./twits/'):\n",
        "    if not os.path.isdir(file):\n",
        "        twits_df = pd.read_csv(file, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "        twits_df = twits_df[twits_df['language'] == 'en']\n",
        "        twits_df = twits_df.reset_index(drop=True)\n",
        "        data = twits_df['tweetid'].tolist()\n",
        "        filepath = str(file.path)\n",
        "        if i == 0:\n",
        "            filelist = []\n",
        "            for d in data:\n",
        "                filelist.append((d,filepath))\n",
        "        else:\n",
        "            for d in data:\n",
        "                filelist.append((d,filepath))\n",
        "        i+=1\n",
        "        if i == 9:\n",
        "            filename = 'twitt_list_' + str(j) + '.txt'\n",
        "            with open(filename, 'w') as f:\n",
        "                for line in filelist:\n",
        "                    f.write(f\"{line}\\n\")\n",
        "            j+=1\n",
        "            i = 0\n",
        "            print(\"Done: \" + str(j*9) + \" / \" + str(total_files))\n",
        "if i != 0: \n",
        "    filename = 'twitt_list_' + str(j) + '.txt'\n",
        "    with open(filename, 'w') as f:\n",
        "        for line in filelist:\n",
        "            f.write(f\"{line}\\n\")\n",
        "    print(\"Done: \" + str(j*9 + i) + \" / \" + str(total_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbdN2KzwDWxb"
      },
      "source": [
        "### Load saved on disk lsh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDs6nrUkDWxb"
      },
      "outputs": [],
      "source": [
        "lsh_s = None\n",
        "for file in os.scandir('./'):\n",
        "    if 'lsh_s_' in file.name:\n",
        "        if lsh_s is None:\n",
        "            lsh_s = sc.textFile(file.path).map(lambda x: x.split('[')).map(lambda x: (re.findall(r'\\d+', x[0]),re.findall(r'\\d+', x[1]))).map(lambda x: (int(x[0][0]),x[1]))\n",
        "        else:\n",
        "            lsh_s1 = sc.textFile(file.path).map(lambda x: x.split('[')).map(lambda x: (re.findall(r'\\d+', x[0]),re.findall(r'\\d+', x[1]))).map(lambda x: (int(x[0][0]),x[1]))\n",
        "            lsh_s = lsh_s.union(lsh_s1).reduceByKey(lambda x, y: x + y)\n",
        "lsh_s = lsh_s.sortBy(lambda x: x[0])\n",
        "#collect_lsh_s = lsh_s.cache().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeJdKEvSDWxc"
      },
      "source": [
        "Load list of twitts and files that contains it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuLcA8UYDWxc"
      },
      "outputs": [],
      "source": [
        "records = None\n",
        "for file in os.scandir('./'):\n",
        "    if 'twitt_list_' in file.name:\n",
        "        if records is None:\n",
        "            records = sc.textFile(file.path).map(lambda x: x.split(',')).map(lambda x: (re.findall(r'\\d+', x[0]),x[1].split(\"'\")[1])).map(lambda x: (int(x[0][0]),x[1]))\n",
        "        else:\n",
        "            records1 = sc.textFile(file.path).map(lambda x: x.split(',')).map(lambda x: (re.findall(r'\\d+', x[0]),x[1].split(\"'\")[1])).map(lambda x: (int(x[0][0]),x[1]))\n",
        "            records = records.union(records1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHiK_lpoDWxc"
      },
      "source": [
        "### Optional: search simmilarity by known twitt id (requires list of twitts and files that contains it as a rdd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfAecbVwDWxc"
      },
      "source": [
        "Load text by twitt ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN0UrLTIDWxc"
      },
      "outputs": [],
      "source": [
        "def load_text_byID(twitt_id, records):\n",
        "    filepath = records.lookup(twitt_id)[0]\n",
        "    twits_df = pd.read_csv(filepath, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "    return twits_df[twits_df['tweetid'] == twitt_id].iloc[0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RW45Ot16DWxc",
        "outputId": "fd23c817-23c4-4c46-a743-134d53320c1e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Fantastic. 👏🏻👏🏻👏🏻👏🏻👏🏻👏🏻👏🏻👏🏻\\n#SlavaUkraini #StandWithUkraine ✊🏻 \\nhttps://t.co/fOx1PKbnnI'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "load_text_byID(1561012415461998603, records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiMWG7amDWxd"
      },
      "source": [
        "Search similar twitts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcBghDR9DWxd"
      },
      "outputs": [],
      "source": [
        "def search_and_print(tweet_id, tweet_text, lsh_s, records, similarity):\n",
        "    if similarity > BANDS:\n",
        "        print(\"Maximum similarity cannot exeed number of bashes: \" + str(BANDS))\n",
        "        return None\n",
        "    search_signature = (tweet_id,lsh(signature(n_gram(tokenize(tweet_text),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "    print('target signature:')\n",
        "    print(search_signature)\n",
        "    answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=similarity).collect()\n",
        "    print('list of candidate tweets:')\n",
        "    print(answer)\n",
        "    print('- - - - - - - - - - - -')\n",
        "    print('original tweet:')\n",
        "    print('id: ' + str(tweet_id))\n",
        "    print('text: ' + tweet_text)\n",
        "    print('=====================')\n",
        "    for ans in answer:\n",
        "        if tweet_id == int(ans[0]): #do not print target tweet\n",
        "            continue\n",
        "        print('sim to ' + str(tweet_id) + ' and ' + str(ans[0]) + ': ' + str(ans[1]))\n",
        "        text = load_text_byID(int(ans[0]), records)\n",
        "        print('Jaccard: ' + str(jaccard(tokenize(tweet_text),tokenize(text))))\n",
        "        print(text)\n",
        "        print('--------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdeew9j_DWxd",
        "outputId": "21e64ea3-eb44-44ef-95d2-2f03a2cc2928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "target signature:\n",
            "(1561010740189974529, [91, 1351, 2271, 3676, 4914, 5473, 6424, 7518, 8707, 9572, 10324, 11411, 12067, 13623, 14449, 15617, 16019, 17089, 18294, 19469])\n",
            "list of candidate tweets:\n",
            "[('1560942577016258561', 20), ('1561010740189974529', 20)]\n",
            "- - - - - - - - - - - -\n",
            "original tweet:\n",
            "id: 1561010740189974529\n",
            "text: #Leaders #AI #transformation\n",
            "🎯#Fox #Business: #China, #Russia 'new world order' is inescapable: Gen. #Keane\n",
            "https://t.co/t59o2WxXue\n",
            "https://t.co/aRAOJLPqaT\n",
            "https://t.co/kJgngbf7lK\n",
            "https://t.co/vwIuYNWYkv\n",
            "https://t.co/YhtzADeKF9\n",
            "https://t.co/g4c0oPZbUW\n",
            "https://t.co/ID11JAo8o0\n",
            "=====================\n",
            "sim to 1561010740189974529 and 1560942577016258561: 20\n",
            "Jaccard: 1.0\n",
            "#Leaders #AI #transformation\n",
            "🎯#Fox #Business: #China, #Russia 'new world order' is inescapable: Gen. #Keane\n",
            "https://t.co/v0LSI1K4n7\n",
            "https://t.co/aPJAt5VGgL\n",
            "https://t.co/xuM07rKJAn\n",
            "https://t.co/MrxjBS3uCF\n",
            "https://t.co/58GMkZa1KB\n",
            "https://t.co/NFgMyVni4t\n",
            "https://t.co/gR3XRl3Ujp\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Search and print text of similar tweets with a target tweetid 1561010740189974529 and at least 5 same buckets\n",
        "search_and_print(1561010740189974529, en_small_twits_df[en_small_twits_df['tweetid'] == 1561010740189974529].iloc[0]['text'], lsh_s, records, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "cYLuYgtlDWxd"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6Qf80_ADWxd",
        "outputId": "c12742d8-90f3-403e-a690-0b9bb06138c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "501.65165519714355\n",
            "20103\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "filename = './twits/0820_UkraineCombinedTweetsDeduped.csv.gzip'\n",
        "small_twits_df = pd.read_csv(filename, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "en_small_twits_df = small_twits_df[small_twits_df['language'] == 'en']\n",
        "en_small_twits_df = en_small_twits_df.reset_index(drop=True)\n",
        "data = en_small_twits_df[['tweetid','text']]\n",
        "#spark = pyspark.sql.SparkSession.builder.appName('pd_to_rdd').getOrCreate()\n",
        "rdd = spark.createDataFrame(data).rdd\n",
        "#rdd.take(3)\n",
        "\n",
        "start_time = time.time()\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),2)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1],N_HASH,MAX_HASH,primes))).map(lambda x: (x[0], lsh(x[1], 20, 1000))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "collect_lsh_s = lsh_s.collect()\n",
        "print((time.time() - start_time))\n",
        "print(rdd.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZirH7PKfDWxd",
        "outputId": "2e9b413e-e510-4926-d419-6bfbb78d9d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "443.23213624954224\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "i = 0\n",
        "for file in os.scandir('./twits/'):\n",
        "    if not os.path.isdir(file):\n",
        "        if '0820' in file.name or '0821' in file.name:\n",
        "            twits_df = pd.read_csv(file, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "            twits_df = twits_df[twits_df['language'] == 'en']\n",
        "            twits_df = twits_df.reset_index(drop=True)\n",
        "            data = twits_df[['tweetid','text']]\n",
        "            if i == 0:\n",
        "                rdd = spark.createDataFrame(data).rdd\n",
        "            else:\n",
        "                rdd1 = spark.createDataFrame(data).rdd\n",
        "                rdd = rdd.union(rdd1)\n",
        "            i+=1\n",
        "        if i == 2:\n",
        "            start_time = time.time()\n",
        "            primes = generate_primes(N_HASH, MAX_HASH)\n",
        "            shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),2)))\n",
        "            lsh_s = shingles.map(lambda x: (x[0],signature(x[1],N_HASH,MAX_HASH,primes))).map(lambda x: (x[0], lsh(x[1], 20, 1000))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "            collect_lsh_s = lsh_s.collect()\n",
        "            print((time.time() - start_time))\n",
        "            break\n",
        "print(rdd.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ5_RLlzDWxe",
        "outputId": "61464a04-f566-449e-8641-2eedb2a5683e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "643.3270268440247\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import time\n",
        "i = 0\n",
        "for file in os.scandir('./twits/'):\n",
        "    if not os.path.isdir(file):\n",
        "        if '0820' in file.name or '0821' in file.name or '0822' in file.name:\n",
        "            twits_df = pd.read_csv(file, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "            twits_df = twits_df[twits_df['language'] == 'en']\n",
        "            twits_df = twits_df.reset_index(drop=True)\n",
        "            data = twits_df[['tweetid','text']]\n",
        "            if i == 0:\n",
        "                rdd = spark.createDataFrame(data).rdd\n",
        "            else:\n",
        "                rdd1 = spark.createDataFrame(data).rdd\n",
        "                rdd = rdd.union(rdd1)\n",
        "            i+=1\n",
        "        if i == 3:\n",
        "            start_time = time.time()\n",
        "            primes = generate_primes(N_HASH, MAX_HASH)\n",
        "            shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),2)))\n",
        "            lsh_s = shingles.map(lambda x: (x[0],signature(x[1],N_HASH,MAX_HASH,primes))).map(lambda x: (x[0], lsh(x[1], 20, 1000))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "            collect_lsh_s = lsh_s.collect()\n",
        "            print((time.time() - start_time))\n",
        "            break\n",
        "print(rdd.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDQ_UmWuDWxe"
      },
      "source": [
        "Count mean of search time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OAqIcyuKDWxe"
      },
      "outputs": [],
      "source": [
        "filename = './twits/0820_UkraineCombinedTweetsDeduped.csv.gzip'\n",
        "small_twits_df = pd.read_csv(filename, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "en_small_twits_df = small_twits_df[small_twits_df['language'] == 'en']\n",
        "en_small_twits_df = en_small_twits_df.reset_index(drop=True)\n",
        "data = en_small_twits_df[['tweetid','text']]\n",
        "#spark = pyspark.sql.SparkSession.builder.appName('pd_to_rdd').getOrCreate()\n",
        "rdd = spark.createDataFrame(data).rdd\n",
        "#rdd.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EouMcD_pDWxe",
        "outputId": "9ce1f421-ab0d-4ff2-c149-a73c714881bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSH calculation time:\n",
            "471.42133688926697\n",
            "Size of LSH cache:\n",
            "168848\n",
            "===========================\n",
            "------------///////////----------\n",
            "Mean singature time /////////// Mean search time\n",
            "0.05450303554534912 0.5206171751022339\n"
          ]
        }
      ],
      "source": [
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "import time\n",
        "start_time = time.time()\n",
        "collect_lsh_s = lsh_s.collect()\n",
        "end_time = time.time() - start_time\n",
        "print(\"LSH calculation time:\")\n",
        "print(end_time)\n",
        "import sys\n",
        "print(\"Size of LSH cache:\")\n",
        "print(sys.getsizeof(collect_lsh_s))\n",
        "\n",
        "print('===========================')\n",
        "tt = 0.0\n",
        "ts = 0.0\n",
        "for i in range(10):\n",
        "    search_record = en_small_twits_df[['tweetid','text']].sample(1).iloc[0]\n",
        "    start_time = time.time()\n",
        "    search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    tt += (end_time - start_time)\n",
        "    start_time = time.time()\n",
        "    answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).collect()\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    ts += (end_time - start_time)\n",
        "    \n",
        "print('------------///////////----------')\n",
        "print('Mean singature time /////////// Mean search time')\n",
        "print((tt/10.0),(ts/10.0))  # Atention: have divider same as range (num of iterations)!\n",
        "#    print(answer)\n",
        "#    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "xKBcwPGmDWxe",
        "outputId": "f83a221d-3cb8-4a65-c098-641003939c8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "218.38458490371704\n",
            "16184\n",
            "===========================\n",
            "0.005997896194458008\n",
            "12.824039936065674\n",
            "0.025026559829711914\n",
            "12.654755115509033\n",
            "0.020982980728149414\n",
            "12.852445602416992\n",
            "0.03099513053894043\n",
            "13.366101026535034\n",
            "0.033976078033447266\n",
            "13.075906753540039\n",
            "0.013999223709106445\n",
            "12.814243793487549\n",
            "0.06599974632263184\n",
            "12.91880178451538\n",
            "0.016972780227661133\n",
            "12.812581777572632\n",
            "0.03101944923400879\n",
            "13.392468214035034\n",
            "0.04499483108520508\n",
            "13.060641765594482\n",
            "0.03202486038208008\n",
            "12.574079275131226\n",
            "0.018025875091552734\n",
            "12.951092720031738\n",
            "0.06099653244018555\n",
            "12.821863412857056\n",
            "0.030998945236206055\n",
            "13.026828527450562\n",
            "0.08499574661254883\n",
            "12.832362651824951\n",
            "0.052971839904785156\n",
            "12.53528618812561\n",
            "0.01898193359375\n",
            "12.512084722518921\n",
            "0.0690000057220459\n",
            "12.553094148635864\n",
            "0.030976057052612305\n",
            "13.08420181274414\n",
            "0.056015968322753906\n",
            "12.64391040802002\n",
            "0.04197549819946289\n",
            "13.098552703857422\n",
            "0.03803062438964844\n",
            "13.001233100891113\n",
            "0.05602884292602539\n",
            "12.677452325820923\n",
            "0.0330195426940918\n",
            "13.208306550979614\n",
            "0.0599980354309082\n",
            "12.654842376708984\n",
            "0.026000499725341797\n",
            "12.74204397201538\n",
            "0.060030221939086914\n",
            "12.73145318031311\n",
            "0.03399825096130371\n",
            "12.74535059928894\n",
            "0.047998905181884766\n",
            "13.019712448120117\n",
            "0.004997968673706055\n",
            "12.773257970809937\n",
            "0.029975175857543945\n",
            "13.000003576278687\n",
            "0.00702667236328125\n",
            "12.644001960754395\n",
            "0.04603314399719238\n",
            "12.554346084594727\n",
            "0.02902960777282715\n",
            "12.9920814037323\n",
            "0.04202985763549805\n",
            "12.435194492340088\n",
            "0.03403210639953613\n",
            "12.502983331680298\n",
            "0.014995813369750977\n",
            "12.805628776550293\n",
            "0.022995948791503906\n",
            "12.748440027236938\n",
            "0.041970014572143555\n",
            "12.9254732131958\n",
            "0.007998943328857422\n",
            "12.754819869995117\n",
            "0.02799844741821289\n",
            "12.983066082000732\n",
            "0.00699925422668457\n",
            "12.57337498664856\n",
            "0.062021493911743164\n",
            "12.534498453140259\n",
            "0.045018672943115234\n",
            "12.506941318511963\n",
            "0.05501675605773926\n",
            "12.995606899261475\n",
            "0.03999733924865723\n",
            "12.519329309463501\n",
            "0.04800009727478027\n",
            "12.835758447647095\n",
            "0.0469970703125\n",
            "12.991986274719238\n",
            "0.03800058364868164\n",
            "13.173970460891724\n",
            "0.05600452423095703\n",
            "12.801048994064331\n",
            "0.02499556541442871\n",
            "12.649142742156982\n",
            "0.056029319763183594\n",
            "12.73504376411438\n",
            "0.009998083114624023\n",
            "12.556056261062622\n",
            "0.062005043029785156\n",
            "13.03701400756836\n",
            "0.048996925354003906\n",
            "12.732009887695312\n",
            "0.05300307273864746\n",
            "12.488649845123291\n",
            "0.023997068405151367\n",
            "12.918519973754883\n",
            "0.05599570274353027\n",
            "12.737445831298828\n",
            "0.040994882583618164\n",
            "12.893739938735962\n",
            "0.01902914047241211\n",
            "13.146883249282837\n",
            "0.027997970581054688\n",
            "12.549933195114136\n",
            "0.02299785614013672\n",
            "12.766556978225708\n",
            "0.05402970314025879\n",
            "12.929891109466553\n",
            "0.012972593307495117\n",
            "12.782267570495605\n",
            "0.024019479751586914\n",
            "12.671125650405884\n",
            "0.008998870849609375\n",
            "12.5333993434906\n",
            "0.027029991149902344\n",
            "12.446386337280273\n",
            "0.04599952697753906\n",
            "12.937463521957397\n",
            "0.003997802734375\n",
            "12.825875997543335\n",
            "0.06999921798706055\n",
            "12.46933889389038\n",
            "0.07700037956237793\n",
            "12.874525547027588\n",
            "0.06399989128112793\n",
            "12.21798062324524\n",
            "0.07704329490661621\n",
            "12.390960216522217\n",
            "0.02099752426147461\n",
            "12.886393547058105\n",
            "0.051996469497680664\n",
            "13.166621446609497\n",
            "0.020993471145629883\n",
            "12.799913883209229\n",
            "0.014997243881225586\n",
            "12.750083684921265\n",
            "0.024997234344482422\n",
            "12.513857126235962\n",
            "0.046996355056762695\n",
            "12.675554752349854\n",
            "0.05901789665222168\n",
            "13.055468082427979\n",
            "0.03400397300720215\n",
            "12.787943840026855\n",
            "0.0679941177368164\n",
            "12.743853330612183\n",
            "0.04199838638305664\n",
            "12.321680068969727\n",
            "0.05202531814575195\n",
            "12.673123598098755\n",
            "0.04899787902832031\n",
            "12.441262245178223\n",
            "0.048998117446899414\n",
            "13.010000467300415\n",
            "0.03602266311645508\n",
            "12.469165086746216\n",
            "0.06499838829040527\n",
            "12.324827432632446\n",
            "0.016013622283935547\n",
            "12.79915976524353\n",
            "0.05803108215332031\n",
            "12.569549798965454\n",
            "0.07203054428100586\n",
            "12.72903037071228\n",
            "0.014999628067016602\n",
            "12.300627708435059\n",
            "0.03699779510498047\n",
            "12.455384254455566\n",
            "0.023996829986572266\n",
            "12.798047304153442\n",
            "0.05298757553100586\n",
            "12.80339002609253\n",
            "0.0029942989349365234\n",
            "12.909505128860474\n",
            "0.034028053283691406\n",
            "12.622595310211182\n",
            "0.04599165916442871\n",
            "12.455677509307861\n",
            "0.04399871826171875\n",
            "12.395315408706665\n",
            "0.059999942779541016\n",
            "12.180513381958008\n",
            "------------///////////----------\n",
            "0.038574085235595704 12.751663038730621\n"
          ]
        }
      ],
      "source": [
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 100 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "import time\n",
        "start_time = time.time()\n",
        "collect_lsh_s = lsh_s.collect()\n",
        "end_time = time.time() - start_time\n",
        "print(\"LSH calculation time:\")\n",
        "print(end_time)\n",
        "import sys\n",
        "print(\"Size of LSH cache:\")\n",
        "print(sys.getsizeof(collect_lsh_s))\n",
        "\n",
        "print('===========================')\n",
        "tt = 0.0\n",
        "ts = 0.0\n",
        "for i in range(100):\n",
        "    search_record = en_small_twits_df[['tweetid','text']].sample(1).iloc[0]\n",
        "    start_time = time.time()\n",
        "    search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    tt += (end_time - start_time)\n",
        "    start_time = time.time()\n",
        "    answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).collect()\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    ts += (end_time - start_time)\n",
        "    \n",
        "print('------------///////////----------')\n",
        "print('Mean singature time /////////// Mean search time')\n",
        "print((tt/100.0),(ts/100.0))\n",
        "#    print(answer)\n",
        "#    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "CbP_rk9fDWxf",
        "outputId": "db8d4867-981b-4a74-ddf7-a339e0902832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "421.06221747398376\n",
            "173016\n",
            "===========================\n",
            "0.11499977111816406\n",
            "12.673866987228394\n",
            "0.12402653694152832\n",
            "12.437915802001953\n",
            "0.09903120994567871\n",
            "12.366593837738037\n",
            "0.1289989948272705\n",
            "12.523196458816528\n",
            "0.08100771903991699\n",
            "12.91687536239624\n",
            "0.11900067329406738\n",
            "12.628182888031006\n",
            "0.02602672576904297\n",
            "12.472917318344116\n",
            "0.4389984607696533\n",
            "12.349596500396729\n",
            "0.014998912811279297\n",
            "12.46495795249939\n",
            "0.03800010681152344\n",
            "12.784342050552368\n",
            "0.07899165153503418\n",
            "12.593489170074463\n",
            "0.06899642944335938\n",
            "12.63657832145691\n",
            "0.1030282974243164\n",
            "12.68916630744934\n",
            "0.11502885818481445\n",
            "12.343005657196045\n",
            "0.10002756118774414\n",
            "12.850990295410156\n",
            "0.03499960899353027\n",
            "12.759755373001099\n",
            "0.08701586723327637\n",
            "12.456097841262817\n",
            "0.07700133323669434\n",
            "12.868533611297607\n",
            "0.05202913284301758\n",
            "12.75425672531128\n",
            "0.10801935195922852\n",
            "12.548707246780396\n",
            "0.14099931716918945\n",
            "12.484606504440308\n",
            "0.10400247573852539\n",
            "12.596319913864136\n",
            "0.09200072288513184\n",
            "12.699769735336304\n",
            "0.07703518867492676\n",
            "13.01785135269165\n",
            "0.06299829483032227\n",
            "12.809865713119507\n",
            "0.12199902534484863\n",
            "12.586020946502686\n",
            "0.04798746109008789\n",
            "12.509544849395752\n",
            "0.11203384399414062\n",
            "12.401830434799194\n",
            "0.10503077507019043\n",
            "12.276578426361084\n",
            "0.027995824813842773\n",
            "12.518088340759277\n",
            "0.05801558494567871\n",
            "12.780696630477905\n",
            "0.12602901458740234\n",
            "12.397971630096436\n",
            "0.0710289478302002\n",
            "12.633641958236694\n",
            "0.0319981575012207\n",
            "12.44559907913208\n",
            "0.009999275207519531\n",
            "12.339986324310303\n",
            "0.04999876022338867\n",
            "12.775328397750854\n",
            "0.08503270149230957\n",
            "12.777579069137573\n",
            "0.06898951530456543\n",
            "12.638847351074219\n",
            "0.1109762191772461\n",
            "12.257957696914673\n",
            "0.11103415489196777\n",
            "12.309479475021362\n",
            "0.12800002098083496\n",
            "12.6396644115448\n",
            "0.029997587203979492\n",
            "12.806350469589233\n",
            "0.12402892112731934\n",
            "13.146286487579346\n",
            "0.06201767921447754\n",
            "12.888760805130005\n",
            "0.13100194931030273\n",
            "12.465030431747437\n",
            "0.10802698135375977\n",
            "12.988620519638062\n",
            "0.039998531341552734\n",
            "12.510842561721802\n",
            "0.05597877502441406\n",
            "12.622544050216675\n",
            "0.025997161865234375\n",
            "12.462357521057129\n",
            "0.09503412246704102\n",
            "12.283594608306885\n",
            "0.03899812698364258\n",
            "12.269363403320312\n",
            "0.07802772521972656\n",
            "13.041096925735474\n",
            "0.1580054759979248\n",
            "12.722270011901855\n",
            "0.04100155830383301\n",
            "12.243328332901001\n",
            "0.13798999786376953\n",
            "12.309080123901367\n",
            "0.024997711181640625\n",
            "12.839216947555542\n",
            "0.0869894027709961\n",
            "12.938685655593872\n",
            "0.06200218200683594\n",
            "12.673694849014282\n",
            "0.05699777603149414\n",
            "12.618777990341187\n",
            "0.05699944496154785\n",
            "12.779501914978027\n",
            "0.02499675750732422\n",
            "12.499331712722778\n",
            "0.14101457595825195\n",
            "13.092146873474121\n",
            "0.049997806549072266\n",
            "12.64175009727478\n",
            "0.11498427391052246\n",
            "12.519753217697144\n",
            "0.11139178276062012\n",
            "12.482023239135742\n",
            "0.06000471115112305\n",
            "12.57500433921814\n",
            "0.08800959587097168\n",
            "12.6590735912323\n",
            "0.1560201644897461\n",
            "12.476187705993652\n",
            "0.03201556205749512\n",
            "12.590432405471802\n",
            "0.10799908638000488\n",
            "12.775551795959473\n",
            "0.15403056144714355\n",
            "12.457097291946411\n",
            "0.018022775650024414\n",
            "12.652736902236938\n",
            "0.07399129867553711\n",
            "12.761407852172852\n",
            "0.07398509979248047\n",
            "12.805030584335327\n",
            "0.04302692413330078\n",
            "12.47871470451355\n",
            "0.1379995346069336\n",
            "12.624635934829712\n",
            "0.1139678955078125\n",
            "12.744183778762817\n",
            "0.10501646995544434\n",
            "13.350661277770996\n",
            "0.12999653816223145\n",
            "12.656604051589966\n",
            "0.12003493309020996\n",
            "12.724732637405396\n",
            "0.05199766159057617\n",
            "12.566946029663086\n",
            "0.11002707481384277\n",
            "12.371811389923096\n",
            "0.11399316787719727\n",
            "12.986142635345459\n",
            "0.09702754020690918\n",
            "12.610793113708496\n",
            "0.08002185821533203\n",
            "12.928078174591064\n",
            "0.04302716255187988\n",
            "12.392769575119019\n",
            "0.0469973087310791\n",
            "12.956263542175293\n",
            "0.12603020668029785\n",
            "13.061813354492188\n",
            "0.14200234413146973\n",
            "12.554364681243896\n",
            "0.10002636909484863\n",
            "13.014230966567993\n",
            "0.05399775505065918\n",
            "12.351963758468628\n",
            "0.07402944564819336\n",
            "12.68420696258545\n",
            "0.032004356384277344\n",
            "12.801817655563354\n",
            "0.03599429130554199\n",
            "12.535857200622559\n",
            "0.11800241470336914\n",
            "12.209821701049805\n",
            "0.10703110694885254\n",
            "13.099927425384521\n",
            "0.08899760246276855\n",
            "12.755910396575928\n",
            "0.11200308799743652\n",
            "13.05114459991455\n",
            "0.11595296859741211\n",
            "12.664923191070557\n",
            "0.0319976806640625\n",
            "12.451329469680786\n",
            "------------///////////----------\n",
            "0.08705171346664428 12.642428333759307\n"
          ]
        }
      ],
      "source": [
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 200 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "import time\n",
        "start_time = time.time()\n",
        "collect_lsh_s = lsh_s.collect()\n",
        "end_time = time.time() - start_time\n",
        "print(\"LSH calculation time:\")\n",
        "print(end_time)\n",
        "import sys\n",
        "print(\"Size of LSH cache:\")\n",
        "print(sys.getsizeof(collect_lsh_s))\n",
        "\n",
        "print('===========================')\n",
        "tt = 0.0\n",
        "ts = 0.0\n",
        "for i in range(100):\n",
        "    search_record = en_small_twits_df[['tweetid','text']].sample(1).iloc[0]\n",
        "    start_time = time.time()\n",
        "    search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    tt += (end_time - start_time)\n",
        "    start_time = time.time()\n",
        "    answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).collect()\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    ts += (end_time - start_time)\n",
        "    \n",
        "print('------------///////////----------')\n",
        "print('Mean singature time /////////// Mean search time')\n",
        "print((tt/100.0),(ts/100.0))\n",
        "#    print(answer)\n",
        "#    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "v7JFi5wdDWxf",
        "outputId": "c38296fb-d99d-4fd3-f58b-67cee333da72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "220.5789942741394\n",
            "85176\n",
            "===========================\n",
            "0.07200121879577637\n",
            "13.114999294281006\n",
            "0.030002832412719727\n",
            "12.992558002471924\n",
            "0.05802154541015625\n",
            "12.86987566947937\n",
            "0.0200197696685791\n",
            "12.529254674911499\n",
            "0.024971485137939453\n",
            "12.411368370056152\n",
            "0.06303286552429199\n",
            "12.712156534194946\n",
            "0.09300112724304199\n",
            "12.820615291595459\n",
            "0.05500316619873047\n",
            "12.890031337738037\n",
            "0.022993087768554688\n",
            "12.472544193267822\n",
            "0.05899858474731445\n",
            "12.582959175109863\n",
            "0.014000892639160156\n",
            "12.800443649291992\n",
            "0.04201984405517578\n",
            "13.11407732963562\n",
            "0.06399941444396973\n",
            "12.51577615737915\n",
            "0.07901334762573242\n",
            "12.505379915237427\n",
            "0.041024208068847656\n",
            "12.612588167190552\n",
            "0.027996540069580078\n",
            "12.617884397506714\n",
            "0.004997968673706055\n",
            "13.208641767501831\n",
            "0.025002002716064453\n",
            "12.620797395706177\n",
            "0.0490264892578125\n",
            "12.35099720954895\n",
            "0.010020732879638672\n",
            "12.655038833618164\n",
            "0.010029315948486328\n",
            "12.692516803741455\n",
            "0.020996809005737305\n",
            "13.178159952163696\n",
            "0.04397082328796387\n",
            "12.700817346572876\n",
            "0.042029380798339844\n",
            "12.649909257888794\n",
            "0.04099631309509277\n",
            "12.67753291130066\n",
            "0.04399681091308594\n",
            "13.093693733215332\n",
            "0.069000244140625\n",
            "13.001917123794556\n",
            "0.014000177383422852\n",
            "12.640061378479004\n",
            "0.04099631309509277\n",
            "12.86978006362915\n",
            "0.018980741500854492\n",
            "12.82561469078064\n",
            "0.04101991653442383\n",
            "12.952705144882202\n",
            "0.0449979305267334\n",
            "13.120183944702148\n",
            "0.030996084213256836\n",
            "13.023939847946167\n",
            "0.047023773193359375\n",
            "12.731773376464844\n",
            "0.05803275108337402\n",
            "12.960472106933594\n",
            "0.027001619338989258\n",
            "12.901305198669434\n",
            "0.05703282356262207\n",
            "12.460018634796143\n",
            "0.07699894905090332\n",
            "13.209942817687988\n",
            "0.06000542640686035\n",
            "12.736705541610718\n",
            "0.023998737335205078\n",
            "12.880606174468994\n",
            "0.05399823188781738\n",
            "12.952641725540161\n",
            "0.05202174186706543\n",
            "12.929882049560547\n",
            "0.026027202606201172\n",
            "13.020489931106567\n",
            "0.022998809814453125\n",
            "12.475236892700195\n",
            "0.026994705200195312\n",
            "12.894354104995728\n",
            "0.010025978088378906\n",
            "12.731150388717651\n",
            "0.024997472763061523\n",
            "12.36943244934082\n",
            "0.009998559951782227\n",
            "13.152075290679932\n",
            "0.0599980354309082\n",
            "12.774907112121582\n",
            "0.04803586006164551\n",
            "12.505961656570435\n",
            "0.060997962951660156\n",
            "12.83654522895813\n",
            "0.08102822303771973\n",
            "12.222102642059326\n",
            "0.010994911193847656\n",
            "13.213813543319702\n",
            "0.024997949600219727\n",
            "12.567955732345581\n",
            "0.06500077247619629\n",
            "12.803901195526123\n",
            "0.05297279357910156\n",
            "12.680840492248535\n",
            "0.0210268497467041\n",
            "12.466213941574097\n",
            "0.04497194290161133\n",
            "12.782201051712036\n",
            "0.05802798271179199\n",
            "12.394670009613037\n",
            "0.04902839660644531\n",
            "12.381386756896973\n",
            "0.0469975471496582\n",
            "12.9124915599823\n",
            "0.03599715232849121\n",
            "12.560478925704956\n",
            "0.07999873161315918\n",
            "12.554391622543335\n",
            "0.037999629974365234\n",
            "12.83352279663086\n",
            "0.010994195938110352\n",
            "12.55974793434143\n",
            "0.05099606513977051\n",
            "12.886784553527832\n",
            "0.04700183868408203\n",
            "12.589713335037231\n",
            "0.0429987907409668\n",
            "12.491674900054932\n",
            "0.061971187591552734\n",
            "13.172754049301147\n",
            "0.02203083038330078\n",
            "12.49622368812561\n",
            "0.07998538017272949\n",
            "12.228989362716675\n",
            "0.03302597999572754\n",
            "12.448131561279297\n",
            "0.027980327606201172\n",
            "12.483878374099731\n",
            "0.05200004577636719\n",
            "13.046742677688599\n",
            "0.06197071075439453\n",
            "12.955267906188965\n",
            "0.025968551635742188\n",
            "12.859413862228394\n",
            "0.011999368667602539\n",
            "12.304901361465454\n",
            "0.0610044002532959\n",
            "12.751026630401611\n",
            "0.0579986572265625\n",
            "12.780831336975098\n",
            "0.06598329544067383\n",
            "12.310432434082031\n",
            "0.01702141761779785\n",
            "12.590357780456543\n",
            "0.060997724533081055\n",
            "12.386797666549683\n",
            "0.023026704788208008\n",
            "12.181433200836182\n",
            "0.035997629165649414\n",
            "12.587383031845093\n",
            "0.05099654197692871\n",
            "12.58029842376709\n",
            "0.008974790573120117\n",
            "12.408660650253296\n",
            "0.02699732780456543\n",
            "12.490990400314331\n",
            "0.051996707916259766\n",
            "12.301262140274048\n",
            "0.05599856376647949\n",
            "12.565902948379517\n",
            "0.0750277042388916\n",
            "12.849036455154419\n",
            "0.030998945236206055\n",
            "12.499122858047485\n",
            "0.012987375259399414\n",
            "12.217379093170166\n",
            "0.03903007507324219\n",
            "12.302051067352295\n",
            "0.039998531341552734\n",
            "12.80490779876709\n",
            "0.11499762535095215\n",
            "12.90194821357727\n",
            "0.02999591827392578\n",
            "12.414757013320923\n",
            "0.030968666076660156\n",
            "12.35963249206543\n",
            "0.05399680137634277\n",
            "12.384473323822021\n",
            "0.024998903274536133\n",
            "12.499972343444824\n",
            "0.04299640655517578\n",
            "12.939117193222046\n",
            "------------///////////----------\n",
            "0.04214303493499756 12.693542885780335\n"
          ]
        }
      ],
      "source": [
        "BANDS = 10 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "import time\n",
        "start_time = time.time()\n",
        "collect_lsh_s = lsh_s.collect()\n",
        "end_time = time.time() - start_time\n",
        "print(\"LSH calculation time:\")\n",
        "print(end_time)\n",
        "import sys\n",
        "print(\"Size of LSH cache:\")\n",
        "print(sys.getsizeof(collect_lsh_s))\n",
        "\n",
        "print('===========================')\n",
        "tt = 0.0\n",
        "ts = 0.0\n",
        "for i in range(100):\n",
        "    search_record = en_small_twits_df[['tweetid','text']].sample(1).iloc[0]\n",
        "    start_time = time.time()\n",
        "    search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    tt += (end_time - start_time)\n",
        "    start_time = time.time()\n",
        "    answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).collect()\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    ts += (end_time - start_time)\n",
        "    \n",
        "print('------------///////////----------')\n",
        "print('Mean singature time /////////// Mean search time')\n",
        "print((tt/100.0),(ts/100.0))\n",
        "#    print(answer)\n",
        "#    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "ocqunySKDWxf",
        "outputId": "12b169f4-6bfd-4993-f9f8-6d677dd1ca19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "220.43481874465942\n",
            "219064\n",
            "===========================\n",
            "0.011996746063232422\n",
            "12.762577533721924\n",
            "0.05599856376647949\n",
            "12.55783486366272\n",
            "0.026021480560302734\n",
            "12.510501146316528\n",
            "0.06102156639099121\n",
            "12.635893106460571\n",
            "0.03299736976623535\n",
            "12.550935983657837\n",
            "0.020974159240722656\n",
            "12.498881816864014\n",
            "0.02300405502319336\n",
            "12.275146484375\n",
            "0.024998903274536133\n",
            "13.70438528060913\n",
            "0.020998477935791016\n",
            "12.96507453918457\n",
            "0.05899477005004883\n",
            "12.351153135299683\n",
            "0.046977996826171875\n",
            "12.386858224868774\n",
            "0.04101896286010742\n",
            "12.865129709243774\n",
            "0.01902031898498535\n",
            "12.367221117019653\n",
            "0.03403043746948242\n",
            "12.986273765563965\n",
            "0.03299832344055176\n",
            "12.743830442428589\n",
            "0.05200052261352539\n",
            "12.447450876235962\n",
            "0.055030107498168945\n",
            "12.580687046051025\n",
            "0.028025150299072266\n",
            "12.360053539276123\n",
            "0.008005380630493164\n",
            "12.880360126495361\n",
            "0.01802849769592285\n",
            "12.284582138061523\n",
            "0.03699779510498047\n",
            "12.598032474517822\n",
            "0.048998355865478516\n",
            "12.519467830657959\n",
            "0.08199739456176758\n",
            "12.494811534881592\n",
            "0.042998313903808594\n",
            "13.169189929962158\n",
            "0.06299996376037598\n",
            "12.874633073806763\n",
            "0.020028114318847656\n",
            "12.589902877807617\n",
            "0.07499933242797852\n",
            "12.779996871948242\n",
            "0.01099848747253418\n",
            "12.82718563079834\n",
            "0.023021697998046875\n",
            "12.807474374771118\n",
            "0.012997150421142578\n",
            "12.493197679519653\n",
            "0.024995803833007812\n",
            "12.629225969314575\n",
            "0.05097007751464844\n",
            "12.472604990005493\n",
            "0.025981664657592773\n",
            "12.371736288070679\n",
            "0.03802967071533203\n",
            "12.505065679550171\n",
            "0.06102561950683594\n",
            "12.903878450393677\n",
            "0.012997865676879883\n",
            "12.311344385147095\n",
            "0.025996685028076172\n",
            "12.455077648162842\n",
            "0.012998580932617188\n",
            "12.636185646057129\n",
            "0.06499719619750977\n",
            "12.386544942855835\n",
            "0.054026126861572266\n",
            "12.982346296310425\n",
            "0.0560302734375\n",
            "12.853326082229614\n",
            "0.04600834846496582\n",
            "12.619413375854492\n",
            "0.016002893447875977\n",
            "12.280508041381836\n",
            "0.041999101638793945\n",
            "12.638036012649536\n",
            "0.04202699661254883\n",
            "12.83669900894165\n",
            "0.04399847984313965\n",
            "12.445375442504883\n",
            "0.03799796104431152\n",
            "12.67937445640564\n",
            "0.039998531341552734\n",
            "12.398631811141968\n",
            "0.06599116325378418\n",
            "12.536112070083618\n",
            "0.07600998878479004\n",
            "12.624136209487915\n",
            "0.040024757385253906\n",
            "12.879974603652954\n",
            "0.0609743595123291\n",
            "12.671489000320435\n",
            "0.02303004264831543\n",
            "12.45149564743042\n",
            "0.01299738883972168\n",
            "12.509078025817871\n",
            "0.026996612548828125\n",
            "12.660082340240479\n",
            "0.0660085678100586\n",
            "13.04287838935852\n",
            "0.055998802185058594\n",
            "12.452476978302002\n",
            "0.027998685836791992\n",
            "12.607920408248901\n",
            "0.018994808197021484\n",
            "12.478073596954346\n",
            "0.06499719619750977\n",
            "12.812100887298584\n",
            "0.04002976417541504\n",
            "13.169018507003784\n",
            "0.04799461364746094\n",
            "12.944411277770996\n",
            "0.0359959602355957\n",
            "12.786569833755493\n",
            "0.018024444580078125\n",
            "12.873162508010864\n",
            "0.08099937438964844\n",
            "13.090141296386719\n",
            "0.05599641799926758\n",
            "13.053795576095581\n",
            "0.02599954605102539\n",
            "12.397263288497925\n",
            "0.03899860382080078\n",
            "12.441648006439209\n",
            "0.06902027130126953\n",
            "13.016386985778809\n",
            "0.027998685836791992\n",
            "12.875876903533936\n",
            "0.05699944496154785\n",
            "13.098886966705322\n",
            "0.01203298568725586\n",
            "12.60041356086731\n",
            "0.036997318267822266\n",
            "12.617438077926636\n",
            "0.026000022888183594\n",
            "13.041823863983154\n",
            "0.030995607376098633\n",
            "12.482547998428345\n",
            "0.04803180694580078\n",
            "13.194077253341675\n",
            "0.020000934600830078\n",
            "12.98109245300293\n",
            "0.040997982025146484\n",
            "12.894465208053589\n",
            "0.05800056457519531\n",
            "12.653535604476929\n",
            "0.0029993057250976562\n",
            "12.73191237449646\n",
            "0.049997568130493164\n",
            "13.04567003250122\n",
            "0.01600480079650879\n",
            "12.422096490859985\n",
            "0.06699991226196289\n",
            "12.401484251022339\n",
            "0.09199309349060059\n",
            "12.358653783798218\n",
            "0.04399824142456055\n",
            "12.671827793121338\n",
            "0.01300048828125\n",
            "12.52053165435791\n",
            "0.019997835159301758\n",
            "12.716532707214355\n",
            "0.060028791427612305\n",
            "12.528266906738281\n",
            "0.06500530242919922\n",
            "12.813125610351562\n",
            "0.05203366279602051\n",
            "12.736066579818726\n",
            "0.06000018119812012\n",
            "12.772271871566772\n",
            "0.028002262115478516\n",
            "12.89673137664795\n",
            "0.05901956558227539\n",
            "12.815113544464111\n",
            "0.04999661445617676\n",
            "12.7381911277771\n",
            "0.05802631378173828\n",
            "12.624491930007935\n",
            "0.06802988052368164\n",
            "12.416066646575928\n",
            "0.008977651596069336\n",
            "13.22641396522522\n",
            "0.02799820899963379\n",
            "12.820292711257935\n",
            "0.024994850158691406\n",
            "12.886605978012085\n",
            "0.0409998893737793\n",
            "12.814692974090576\n",
            "------------///////////----------\n",
            "0.04003501415252685 12.691695113182067\n"
          ]
        }
      ],
      "source": [
        "BANDS = 25 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "import time\n",
        "start_time = time.time()\n",
        "collect_lsh_s = lsh_s.collect()\n",
        "end_time = time.time() - start_time\n",
        "print(\"LSH calculation time:\")\n",
        "print(end_time)\n",
        "import sys\n",
        "print(\"Size of LSH cache:\")\n",
        "print(sys.getsizeof(collect_lsh_s))\n",
        "\n",
        "print('===========================')\n",
        "tt = 0.0\n",
        "ts = 0.0\n",
        "for i in range(100):\n",
        "    search_record = en_small_twits_df[['tweetid','text']].sample(1).iloc[0]\n",
        "    start_time = time.time()\n",
        "    search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    tt += (end_time - start_time)\n",
        "    start_time = time.time()\n",
        "    answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).collect()\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    ts += (end_time - start_time)\n",
        "    \n",
        "print('------------///////////----------')\n",
        "print('Mean singature time /////////// Mean search time')\n",
        "print((tt/100.0),(ts/100.0))\n",
        "#    print(answer)\n",
        "#    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "w8r0LrIUDWxf",
        "outputId": "9cf6e44a-0eca-4d7e-e52a-bb9aa4f0920f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "139.33454084396362\n",
            "173016\n",
            "===========================\n",
            "0.01201009750366211\n",
            "12.977538347244263\n",
            "0.0069959163665771484\n",
            "12.708106517791748\n",
            "0.00899815559387207\n",
            "12.309601545333862\n",
            "0.03297066688537598\n",
            "12.408851385116577\n",
            "0.007997989654541016\n",
            "12.564412832260132\n",
            "0.01998615264892578\n",
            "12.675443172454834\n",
            "0.00699925422668457\n",
            "12.952386856079102\n",
            "0.034975528717041016\n",
            "12.744630098342896\n",
            "0.007028341293334961\n",
            "12.942699670791626\n",
            "0.006997823715209961\n",
            "12.848982095718384\n",
            "0.015998125076293945\n",
            "12.688977003097534\n",
            "0.029015064239501953\n",
            "13.162891149520874\n",
            "0.03499937057495117\n",
            "12.805161714553833\n",
            "0.030992507934570312\n",
            "12.719957828521729\n",
            "0.03399658203125\n",
            "12.771106719970703\n",
            "0.01800060272216797\n",
            "12.85887336730957\n",
            "0.020972728729248047\n",
            "13.143899202346802\n",
            "0.01696920394897461\n",
            "12.678240537643433\n",
            "0.03999757766723633\n",
            "12.962343454360962\n",
            "0.015028238296508789\n",
            "13.003205060958862\n",
            "0.023999452590942383\n",
            "13.13677453994751\n",
            "0.024999380111694336\n",
            "13.322359561920166\n",
            "0.018000364303588867\n",
            "13.002450942993164\n",
            "0.009996891021728516\n",
            "12.723389625549316\n",
            "0.019024372100830078\n",
            "12.59166932106018\n",
            "0.0300137996673584\n",
            "12.396546602249146\n",
            "0.03499484062194824\n",
            "12.800218343734741\n",
            "0.009989023208618164\n",
            "12.581254243850708\n",
            "0.04096627235412598\n",
            "12.618067264556885\n",
            "0.029999732971191406\n",
            "12.485353708267212\n",
            "0.02399730682373047\n",
            "12.673356771469116\n",
            "0.024029254913330078\n",
            "13.204272031784058\n",
            "0.03799915313720703\n",
            "12.75148892402649\n",
            "0.02599930763244629\n",
            "12.775962829589844\n",
            "0.019000530242919922\n",
            "13.153298616409302\n",
            "0.024985790252685547\n",
            "13.187009572982788\n",
            "0.003998994827270508\n",
            "13.330620765686035\n",
            "0.05699801445007324\n",
            "13.005648374557495\n",
            "0.03699851036071777\n",
            "12.848939418792725\n",
            "0.029998064041137695\n",
            "14.119289875030518\n",
            "0.03699660301208496\n",
            "15.057002305984497\n",
            "0.0339970588684082\n",
            "13.076193571090698\n",
            "0.008997678756713867\n",
            "12.944643020629883\n",
            "0.024028778076171875\n",
            "12.943471431732178\n",
            "0.028998613357543945\n",
            "12.309959888458252\n",
            "0.041994333267211914\n",
            "12.739647626876831\n",
            "0.028034210205078125\n",
            "12.614558696746826\n",
            "0.048029422760009766\n",
            "12.672173261642456\n",
            "0.025997638702392578\n",
            "12.847373962402344\n",
            "0.010996103286743164\n",
            "12.61543869972229\n",
            "0.019028425216674805\n",
            "12.785693883895874\n",
            "0.028998613357543945\n",
            "12.714749097824097\n",
            "0.043032169342041016\n",
            "13.35823678970337\n",
            "0.01799607276916504\n",
            "12.991024494171143\n",
            "0.012997627258300781\n",
            "13.093321800231934\n",
            "0.018998146057128906\n",
            "12.567776679992676\n",
            "0.015999794006347656\n",
            "12.50051236152649\n",
            "0.016030073165893555\n",
            "13.122263669967651\n",
            "0.017000913619995117\n",
            "12.924147605895996\n",
            "0.029024124145507812\n",
            "12.929253816604614\n",
            "0.027996301651000977\n",
            "12.425487041473389\n",
            "0.023954391479492188\n",
            "12.692879915237427\n",
            "0.02402806282043457\n",
            "13.025958776473999\n",
            "0.028996944427490234\n",
            "12.447777509689331\n",
            "0.010999441146850586\n",
            "12.680885314941406\n",
            "0.02302861213684082\n",
            "12.683649778366089\n",
            "0.019019603729248047\n",
            "12.643086910247803\n",
            "0.015993833541870117\n",
            "13.272247314453125\n",
            "0.027027606964111328\n",
            "12.75460147857666\n",
            "0.005982637405395508\n",
            "13.214625120162964\n",
            "0.030998706817626953\n",
            "13.025591373443604\n",
            "0.01799750328063965\n",
            "12.58351755142212\n",
            "0.03599858283996582\n",
            "13.09647536277771\n",
            "0.02200007438659668\n",
            "13.015188455581665\n",
            "0.03898477554321289\n",
            "12.575819492340088\n",
            "0.017001867294311523\n",
            "12.773932933807373\n",
            "0.03599667549133301\n",
            "12.816553592681885\n",
            "0.014997482299804688\n",
            "12.995048761367798\n",
            "0.007998466491699219\n",
            "12.547825574874878\n",
            "0.012022018432617188\n",
            "12.617152214050293\n",
            "0.03699612617492676\n",
            "12.399436235427856\n",
            "0.023998737335205078\n",
            "13.022742509841919\n",
            "0.01699352264404297\n",
            "13.310776710510254\n",
            "0.03401947021484375\n",
            "13.053688287734985\n",
            "0.010998010635375977\n",
            "12.572521448135376\n",
            "0.031026124954223633\n",
            "13.095642805099487\n",
            "0.031998634338378906\n",
            "12.571929931640625\n",
            "0.039997100830078125\n",
            "12.98947525024414\n",
            "0.041001319885253906\n",
            "13.00829267501831\n",
            "0.019021272659301758\n",
            "12.82247805595398\n",
            "0.029975175857543945\n",
            "12.968293190002441\n",
            "0.01898789405822754\n",
            "12.607042789459229\n",
            "0.031028270721435547\n",
            "12.646947383880615\n",
            "0.057999610900878906\n",
            "12.836930513381958\n",
            "0.020445585250854492\n",
            "12.507896423339844\n",
            "0.02899622917175293\n",
            "12.725761413574219\n",
            "0.04000067710876465\n",
            "12.444143295288086\n",
            "0.020999908447265625\n",
            "13.104780673980713\n",
            "0.01802825927734375\n",
            "13.147786855697632\n",
            "0.0319981575012207\n",
            "12.510966300964355\n",
            "------------///////////----------\n",
            "0.024596290588378908 12.856785597801208\n"
          ]
        }
      ],
      "source": [
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 1 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "import time\n",
        "start_time = time.time()\n",
        "collect_lsh_s = lsh_s.collect()\n",
        "end_time = time.time() - start_time\n",
        "print(\"LSH calculation time:\")\n",
        "print(end_time)\n",
        "import sys\n",
        "print(\"Size of LSH cache:\")\n",
        "print(sys.getsizeof(collect_lsh_s))\n",
        "\n",
        "print('===========================')\n",
        "tt = 0.0\n",
        "ts = 0.0\n",
        "for i in range(100):\n",
        "    search_record = en_small_twits_df[['tweetid','text']].sample(1).iloc[0]\n",
        "    start_time = time.time()\n",
        "    search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    tt += (end_time - start_time)\n",
        "    start_time = time.time()\n",
        "    answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).collect()\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    ts += (end_time - start_time)\n",
        "    \n",
        "print('------------///////////----------')\n",
        "print('Mean singature time /////////// Mean search time')\n",
        "print((tt/100.0),(ts/100.0))\n",
        "#    print(answer)\n",
        "#    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "bfBu9hAADWxg",
        "outputId": "d35cbe7c-1784-4994-a31b-43a368c1eb15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "292.1011734008789\n",
            "173016\n",
            "===========================\n",
            "0.07999634742736816\n",
            "12.367831707000732\n",
            "0.05499839782714844\n",
            "13.085931301116943\n",
            "0.08302807807922363\n",
            "12.545076370239258\n",
            "0.07399415969848633\n",
            "12.64473032951355\n",
            "0.026998519897460938\n",
            "12.709683895111084\n",
            "0.025026321411132812\n",
            "12.571649551391602\n",
            "0.04899859428405762\n",
            "13.512852430343628\n",
            "0.04797053337097168\n",
            "12.911753177642822\n",
            "0.00699925422668457\n",
            "12.818846702575684\n",
            "0.05999875068664551\n",
            "12.87521767616272\n",
            "0.02397298812866211\n",
            "13.029248714447021\n",
            "0.040000200271606445\n",
            "13.135928630828857\n",
            "0.06699037551879883\n",
            "12.937897205352783\n",
            "0.02902531623840332\n",
            "12.992859840393066\n",
            "0.05302691459655762\n",
            "13.000807523727417\n",
            "0.062033653259277344\n",
            "12.991647243499756\n",
            "0.05699753761291504\n",
            "13.011254787445068\n",
            "0.06302928924560547\n",
            "12.651792287826538\n",
            "0.04402518272399902\n",
            "12.719950199127197\n",
            "0.0670015811920166\n",
            "12.976734399795532\n",
            "0.024998903274536133\n",
            "12.935436010360718\n",
            "0.010996103286743164\n",
            "13.087275266647339\n",
            "0.06799769401550293\n",
            "12.483688831329346\n",
            "0.08299756050109863\n",
            "12.765823125839233\n",
            "0.0410306453704834\n",
            "12.611506938934326\n",
            "0.015027999877929688\n",
            "12.994558095932007\n",
            "0.028002023696899414\n",
            "13.16912031173706\n",
            "0.09099841117858887\n",
            "12.9115469455719\n",
            "0.09599828720092773\n",
            "12.929626226425171\n",
            "0.03902482986450195\n",
            "12.39102554321289\n",
            "0.09402179718017578\n",
            "12.834314107894897\n",
            "0.061998844146728516\n",
            "13.194389343261719\n",
            "0.05402946472167969\n",
            "12.69568657875061\n",
            "0.07799887657165527\n",
            "12.920212745666504\n",
            "0.012999534606933594\n",
            "12.750975370407104\n",
            "0.02699732780456543\n",
            "12.885552883148193\n",
            "0.08202743530273438\n",
            "13.18384838104248\n",
            "0.05599856376647949\n",
            "13.176374673843384\n",
            "0.0730290412902832\n",
            "13.002792596817017\n",
            "0.014998674392700195\n",
            "12.745196104049683\n",
            "0.036022186279296875\n",
            "13.140152215957642\n",
            "0.023996353149414062\n",
            "13.056347370147705\n",
            "0.03102731704711914\n",
            "13.229055881500244\n",
            "0.021021127700805664\n",
            "13.159141778945923\n",
            "0.021999120712280273\n",
            "12.598369359970093\n",
            "0.0710148811340332\n",
            "12.645025253295898\n",
            "0.024998188018798828\n",
            "13.183104038238525\n",
            "0.07002854347229004\n",
            "12.69484257698059\n",
            "0.0919950008392334\n",
            "12.543359279632568\n",
            "0.07000064849853516\n",
            "12.779930114746094\n",
            "0.04602861404418945\n",
            "12.79622769355774\n",
            "0.09399938583374023\n",
            "13.126618385314941\n",
            "0.03699493408203125\n",
            "13.269898176193237\n",
            "0.027025938034057617\n",
            "12.713252782821655\n",
            "0.0919952392578125\n",
            "12.836281776428223\n",
            "0.026998519897460938\n",
            "13.004267692565918\n",
            "0.08802628517150879\n",
            "12.360457181930542\n",
            "0.03600740432739258\n",
            "13.126204490661621\n",
            "0.005000114440917969\n",
            "12.6744384765625\n",
            "0.04901266098022461\n",
            "12.986210584640503\n",
            "0.0279996395111084\n",
            "12.861496925354004\n",
            "0.07401585578918457\n",
            "12.477022886276245\n",
            "0.027996063232421875\n",
            "12.944183588027954\n",
            "0.024033784866333008\n",
            "12.986800193786621\n",
            "0.07299971580505371\n",
            "12.835512638092041\n",
            "0.045977115631103516\n",
            "12.592388153076172\n",
            "0.05497622489929199\n",
            "12.979460000991821\n",
            "0.04602646827697754\n",
            "12.865010499954224\n",
            "0.05299687385559082\n",
            "12.758144855499268\n",
            "0.042995452880859375\n",
            "12.778934001922607\n",
            "0.09103131294250488\n",
            "12.879589319229126\n",
            "0.014999151229858398\n",
            "12.601428270339966\n",
            "0.07602763175964355\n",
            "13.237653017044067\n",
            "0.03536272048950195\n",
            "12.747172832489014\n",
            "0.06899690628051758\n",
            "12.725158929824829\n",
            "0.044999122619628906\n",
            "12.781216144561768\n",
            "0.026955366134643555\n",
            "12.673190832138062\n",
            "0.06002664566040039\n",
            "13.12763786315918\n",
            "0.042997121810913086\n",
            "12.711541891098022\n",
            "0.050997018814086914\n",
            "12.792842149734497\n",
            "0.04400062561035156\n",
            "12.72693133354187\n",
            "0.022016286849975586\n",
            "12.45576524734497\n",
            "0.0049974918365478516\n",
            "12.775184154510498\n",
            "0.08399748802185059\n",
            "12.89012885093689\n",
            "0.04600048065185547\n",
            "12.700978994369507\n",
            "0.0319972038269043\n",
            "12.889456987380981\n",
            "0.09300041198730469\n",
            "12.948110342025757\n",
            "0.03396749496459961\n",
            "12.468832015991211\n",
            "0.08799886703491211\n",
            "12.97244644165039\n",
            "0.009998083114624023\n",
            "12.537420272827148\n",
            "0.07999539375305176\n",
            "12.848902940750122\n",
            "0.0690310001373291\n",
            "13.129963636398315\n",
            "0.07699823379516602\n",
            "12.989674806594849\n",
            "0.08399415016174316\n",
            "13.291195631027222\n",
            "0.07842397689819336\n",
            "12.469630718231201\n",
            "0.07901930809020996\n",
            "12.48303484916687\n",
            "0.0890207290649414\n",
            "12.43860673904419\n",
            "0.01896834373474121\n",
            "12.824782133102417\n",
            "0.03999686241149902\n",
            "12.994203090667725\n",
            "0.05299830436706543\n",
            "12.694388151168823\n",
            "------------///////////----------\n",
            "0.05137307405471802 12.849698514938355\n"
          ]
        }
      ],
      "source": [
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 3 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "import time\n",
        "start_time = time.time()\n",
        "collect_lsh_s = lsh_s.collect()\n",
        "end_time = time.time() - start_time\n",
        "print(\"LSH calculation time:\")\n",
        "print(end_time)\n",
        "import sys\n",
        "print(\"Size of LSH cache:\")\n",
        "print(sys.getsizeof(collect_lsh_s))\n",
        "\n",
        "print('===========================')\n",
        "tt = 0.0\n",
        "ts = 0.0\n",
        "for i in range(100):\n",
        "    search_record = en_small_twits_df[['tweetid','text']].sample(1).iloc[0]\n",
        "    start_time = time.time()\n",
        "    search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    tt += (end_time - start_time)\n",
        "    start_time = time.time()\n",
        "    answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).collect()\n",
        "    end_time = time.time()\n",
        "    #print((end_time - start_time))\n",
        "    ts += (end_time - start_time)\n",
        "    \n",
        "print('------------///////////----------')\n",
        "print('Mean singature time /////////// Mean search time')\n",
        "print((tt/100.0),(ts/100.0))\n",
        "#    print(answer)\n",
        "#    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkE6LYgsDWxg"
      },
      "source": [
        "Manual check of similar tweets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "hLb0cOAfDWxg"
      },
      "outputs": [],
      "source": [
        "filename = './twits/0820_UkraineCombinedTweetsDeduped.csv.gzip'\n",
        "small_twits_df = pd.read_csv(filename, compression='gzip', index_col=0,encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "en_small_twits_df = small_twits_df[small_twits_df['language'] == 'en']\n",
        "en_small_twits_df = en_small_twits_df.reset_index(drop=True)\n",
        "data = en_small_twits_df[['tweetid','text']]\n",
        "#spark = pyspark.sql.SparkSession.builder.appName('pd_to_rdd').getOrCreate()\n",
        "rdd = spark.createDataFrame(data).rdd\n",
        "#rdd.take(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu6y7LwpDWxg"
      },
      "source": [
        "List of tweetid that i used as target over search from 0820 file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUMfTdvODWxg"
      },
      "outputs": [],
      "source": [
        "[1561115380600283136, 1561126718601105410, 1560970279572176896, 1560916346828996608, 1560872710565433344, 1561027561022423043, 1561076241851097091, 1560790174740746243]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ83Tj6LDWxh"
      },
      "source": [
        "1 - original (no FP)\n",
        "\n",
        "2 - more false possitive (much more with sim 3, some additional sim 4 FP)\n",
        "\n",
        "3 - only 100%\n",
        "\n",
        "4 - only 100%\n",
        "\n",
        "5 - found new similar tweet '1561115373801390081'. No FP. Changed values of old one. Miss '1560785746130137088' record\n",
        "\n",
        "6 - Very strong FP results (have max sim)\n",
        "\n",
        "7 - only 100%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E92FmkmDWxh",
        "outputId": "c25b8d03-9bb7-43ae-993d-f200c6ab8d95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 1000 2 100\n",
            "-------------------------\n",
            "[(1560785746130137088, 3, 0.5263157894736842), (1561115380600283136, 20, 1.0), (1560830509982224384, 4, 0.875), (1560853141708365824, 4, 0.7222222222222222)]\n",
            "=========================\n",
            "20 100 2 100\n",
            "-------------------------\n",
            "[(1560785746130137088, 3, 0.5263157894736842), (1560890654053617664, 3, 0.0), (1560893665316261888, 3, 0.0), (1560896386232528896, 3, 0.02631578947368421), (1560982742187364358, 4, 0.02631578947368421), (1561028268442124288, 3, 0.0), (1561099416131768320, 3, 0.0), (1561115380600283136, 20, 1.0), (1560807328617615360, 3, 0.0), (1560950220376227842, 3, 0.0), (1560954586827051010, 3, 0.043478260869565216), (1560960862231228416, 3, 0.0), (1561130774975467526, 3, 0.0), (1560853141708365824, 4, 0.7222222222222222), (1560889581155008512, 3, 0.0), (1561023184874651648, 3, 0.0), (1561060616726581250, 3, 0.0), (1561065324048523264, 3, 0.0), (1560781490635644928, 3, 0.0), (1560830509982224384, 4, 0.875), (1560818517141864448, 3, 0.029411764705882353), (1561133375305424898, 3, 0.0), (1561038229939396608, 3, 0.0), (1560977744267288578, 3, 0.025), (1560852982970793985, 3, 0.0), (1560798902592339975, 3, 0.0), (1561070553598988291, 3, 0.0), (1561005702004592641, 3, 0.0), (1561049398590771207, 3, 0.0), (1560780650986328065, 3, 0.0), (1560946650771853313, 3, 0.0), (1560974823316025347, 3, 0.0), (1560989196117954567, 3, 0.023255813953488372), (1561071013189795843, 3, 0.0), (1560899449844649985, 3, 0.09090909090909091), (1560912967688126465, 3, 0.0)]\n",
            "=========================\n",
            "20 1000 2 200\n",
            "-------------------------\n",
            "[(1561115380600283136, 20, 1.0)]\n",
            "=========================\n",
            "10 1000 2 100\n",
            "-------------------------\n",
            "[(1561115380600283136, 10, 1.0)]\n",
            "=========================\n",
            "25 1000 2 100\n",
            "-------------------------\n",
            "[(1561115380600283136, 25, 1.0), (1560830509982224384, 5, 0.875), (1560853141708365824, 7, 0.7222222222222222), (1561115373801390081, 4, 0.875)]\n",
            "=========================\n",
            "20 1000 1 100\n",
            "-------------------------\n",
            "[(1560778700999692288, 20, 0.024390243902439025), (1560782881454276610, 17, 0.05128205128205128), (1560783679361654784, 18, 0.02702702702702703), (1560785016606625794, 20, 0.02702702702702703), (1560785746130137088, 20, 0.5263157894736842), (1560787465597620224, 20, 0.05714285714285714), (1560790979896016900, 20, 0.045454545454545456), (1560792640530120712, 20, 0.02702702702702703), (1560792680967331842, 20, 0.02702702702702703), (1560795333554503680, 19, 0.025), (1560795398393970688, 18, 0.0), (1560797101918928896, 4, 0.0), (1560800613985918976, 20, 0.041666666666666664), (1560808148470059008, 20, 0.02702702702702703), (1560816636126429184, 20, 0.027777777777777776), (1560817554293022720, 19, 0.02702702702702703), (1560818828191350786, 20, 0.1), (1560820417811578880, 20, 0.047619047619047616), (1560821963274084352, 19, 0.027777777777777776), (1560824313640431616, 6, 0.0), (1560828511572488192, 5, 0.0), (1560830509982224384, 20, 0.875), (1560834986311225344, 19, 0.047619047619047616), (1560835090657296384, 4, 0.0), (1560835672428056576, 20, 0.05263157894736842), (1560839565480718336, 7, 0.023809523809523808), (1560839570261827584, 20, 0.034482758620689655), (1560839708359393280, 17, 0.0), (1560843925287493634, 18, 0.06060606060606061), (1560849175868219392, 9, 0.0), (1560849954729500672, 18, 0.02702702702702703), (1560853141708365824, 20, 0.7222222222222222), (1560854650852614144, 20, 0.045454545454545456), (1560857871750819840, 20, 0.07407407407407407), (1560858147782365190, 20, 0.029411764705882353), (1560861524763152384, 19, 0.09523809523809523), (1560868597219598336, 20, 0.05555555555555555), (1560868742942392320, 20, 0.045454545454545456), (1560875806385135618, 20, 0.05263157894736842), (1560877838923976704, 20, 0.043478260869565216), (1560877890295693312, 4, 0.0), (1560879188047020032, 20, 0.047619047619047616), (1560882065460973568, 20, 0.043478260869565216), (1560883667706138624, 20, 0.037037037037037035), (1560884237430972416, 20, 0.022727272727272728), (1560885894307086336, 19, 0.03125), (1560887979169685504, 5, 0.0), (1560889414343180288, 18, 0.0), (1560891607737454592, 20, 0.0), (1560891814617395200, 20, 0.02631578947368421), (1560896213519855616, 20, 0.02631578947368421), (1560897137868562432, 20, 0.029411764705882353), (1560898273883426818, 19, 0.06451612903225806), (1560900298658533376, 20, 0.05263157894736842), (1560901427584794626, 19, 0.041666666666666664), (1560902763172741120, 19, 0.043478260869565216), (1560903394675806214, 19, 0.041666666666666664), (1560903838118338560, 19, 0.024390243902439025), (1560904105538863104, 20, 0.02631578947368421), (1560905963166736384, 20, 0.02702702702702703), (1560906271699750912, 20, 0.03571428571428571), (1560909650530373634, 20, 0.045454545454545456), (1560911656267522048, 8, 0.0), (1560912799664115712, 20, 0.027777777777777776), (1560914662744547328, 8, 0.0), (1560916075411283970, 20, 0.043478260869565216), (1560916549023989762, 20, 0.05), (1560917712595025922, 20, 0.02564102564102564), (1560920783416877056, 17, 0.030303030303030304), (1560920916485349376, 6, 0.0), (1560922746661519360, 20, 0.029411764705882353), (1560923092834189314, 20, 0.037037037037037035), (1560924302152794112, 19, 0.0), (1560928870890627074, 19, 0.03333333333333333), (1560928883947569156, 14, 0.0), (1560931145809891328, 10, 0.0), (1560931298440626176, 18, 0.0), (1560934922168242176, 13, 0.0), (1560935204436451328, 20, 0.038461538461538464), (1560937601699352576, 20, 0.023255813953488372), (1560940088917479424, 20, 0.02857142857142857), (1560941862336225280, 18, 0.03333333333333333), (1560942605868859394, 10, 0.0), (1560943736577744896, 20, 0.045454545454545456), (1560945584458063872, 20, 0.03225806451612903), (1560954336078962688, 19, 0.029411764705882353), (1560954586827051010, 20, 0.043478260869565216), (1560954901567528960, 8, 0.0), (1560956798059814912, 19, 0.022222222222222223), (1560962431005982720, 20, 0.03571428571428571), (1560963446379728902, 3, 0.0), (1560968080007876610, 20, 0.03125), (1560969185915801600, 20, 0.03225806451612903), (1560969658710327296, 20, 0.045454545454545456), (1560969976818946054, 14, 0.0), (1560971170102517762, 20, 0.045454545454545456), (1560971346670141440, 19, 0.0), (1560974821936201728, 19, 0.0), (1560975784826863618, 20, 0.041666666666666664), (1560977934567067648, 20, 0.05), (1560978213907709958, 20, 0.075), (1560980185234788354, 19, 0.06451612903225806), (1560981077497626624, 20, 0.02702702702702703), (1560982087410323456, 20, 0.05555555555555555), (1560982776710631424, 20, 0.05555555555555555), (1560987021925126144, 20, 0.043478260869565216), (1560989475827744768, 20, 0.045454545454545456), (1560990084312309760, 19, 0.027777777777777776), (1560990595933523972, 18, 0.025), (1560991501739499522, 19, 0.023809523809523808), (1560993446923362304, 20, 0.047619047619047616), (1560993639945363458, 19, 0.02857142857142857), (1560993997937643522, 20, 0.02702702702702703), (1560994482279297024, 19, 0.02564102564102564), (1560996089125376000, 20, 0.08695652173913043), (1560999093307969536, 18, 0.023809523809523808), (1560999287458349056, 19, 0.038461538461538464), (1561001516315856896, 19, 0.030303030303030304), (1561003772452560896, 20, 0.05714285714285714), (1561004033216741376, 20, 0.034482758620689655), (1561004036878467072, 13, 0.0), (1561004131229114368, 18, 0.05), (1561005396466229248, 19, 0.043478260869565216), (1561005491853168640, 19, 0.07692307692307693), (1561005919152005120, 19, 0.0), (1561005961439059968, 16, 0.02857142857142857), (1561010581229928452, 5, 0.025), (1561011887143133184, 19, 0.030303030303030304), (1561013934823555072, 20, 0.02564102564102564), (1561014376546590720, 19, 0.037037037037037035), (1561019738750439424, 20, 0.045454545454545456), (1561021993234927616, 19, 0.034482758620689655), (1561022270490824704, 19, 0.03125), (1561022531049558016, 18, 0.030303030303030304), (1561023905363525632, 19, 0.029411764705882353), (1561023963437928448, 18, 0.0), (1561023985227157504, 20, 0.037037037037037035), (1561024164479307776, 11, 0.0), (1561027898089373698, 20, 0.0), (1561028044759994370, 20, 0.045454545454545456), (1561029705754722304, 20, 0.02631578947368421), (1561031813551751168, 19, 0.043478260869565216), (1561033938239455234, 19, 0.02564102564102564), (1561035332816031744, 20, 0.037037037037037035), (1561035538919997440, 19, 0.03225806451612903), (1561035794848092164, 20, 0.04), (1561040524676665348, 19, 0.04), (1561040949941248000, 20, 0.09090909090909091), (1561041161833402368, 19, 0.02702702702702703), (1561041446618075142, 20, 0.041666666666666664), (1561041649563799554, 19, 0.02564102564102564), (1561042055714971648, 18, 0.043478260869565216), (1561044560566013956, 20, 0.03225806451612903), (1561046078035746818, 20, 0.034482758620689655), (1561050072397230080, 18, 0.02564102564102564), (1561055958557753344, 18, 0.023809523809523808), (1561057791648927744, 19, 0.037037037037037035), (1561058577753817094, 18, 0.06451612903225806), (1561059900612296704, 19, 0.02857142857142857), (1561061931162681344, 17, 0.02564102564102564), (1561062800662753280, 20, 0.045454545454545456), (1561063080590721024, 19, 0.02857142857142857), (1561064025714266112, 18, 0.06451612903225806), (1561066850523287552, 19, 0.037037037037037035), (1561067856233697284, 19, 0.0975609756097561), (1561067965918969858, 20, 0.0), (1561069012410400770, 20, 0.0625), (1561070514944315392, 20, 0.038461538461538464), (1561071600706936832, 20, 0.037037037037037035), (1561071752997965834, 20, 0.03225806451612903), (1561072717876576258, 18, 0.03333333333333333), (1561073034018099200, 20, 0.05263157894736842), (1561076269147717638, 20, 0.045454545454545456), (1561078714397515778, 20, 0.03225806451612903), (1561080552941404160, 20, 0.0), (1561081209853284358, 10, 0.0), (1561082176170754048, 12, 0.0), (1561082662042861568, 5, 0.025), (1561082946630582274, 19, 0.047619047619047616), (1561083178948902914, 19, 0.02857142857142857), (1561083503587971074, 20, 0.02631578947368421), (1561085016188534784, 18, 0.034482758620689655), (1561087401467994118, 14, 0.045454545454545456), (1561088057125834752, 19, 0.02702702702702703), (1561088767125245952, 20, 0.030303030303030304), (1561090599209648132, 20, 0.027777777777777776), (1561090905343393792, 20, 0.06060606060606061), (1561092541402972162, 20, 0.0), (1561094079013945348, 20, 0.025), (1561094362532020224, 17, 0.02564102564102564), (1561094613640925186, 20, 0.037037037037037035), (1561094626374819840, 19, 0.08333333333333333), (1561095695662186498, 19, 0.027777777777777776), (1561096214963073024, 18, 0.05714285714285714), (1561102454288433152, 20, 0.02631578947368421), (1561102887874338818, 20, 0.02702702702702703), (1561104493940457472, 20, 0.05263157894736842), (1561105277218426880, 16, 0.02702702702702703), (1561106899868819458, 20, 0.03125), (1561108332869554176, 20, 0.05), (1561110462804201472, 20, 0.07692307692307693), (1561113709577125888, 20, 0.038461538461538464), (1561114351771078656, 10, 0.0), (1561114871349100544, 20, 0.030303030303030304), (1561115216489979906, 20, 0.05555555555555555), (1561115380600283136, 20, 1.0), (1561116299933220866, 20, 0.03571428571428571), (1561119268862828546, 20, 0.047619047619047616), (1561119708132384768, 20, 0.05555555555555555), (1561121157671182336, 20, 0.045454545454545456), (1561121225887211520, 19, 0.02857142857142857), (1561122759085785088, 19, 0.045454545454545456), (1561123398759104518, 20, 0.02564102564102564), (1561124283337871362, 10, 0.0), (1561126685440876544, 17, 0.037037037037037035), (1561127158805831680, 19, 0.030303030303030304), (1561129437793484800, 20, 0.043478260869565216), (1561132663578263552, 6, 0.0), (1561133000275943424, 19, 0.02564102564102564), (1561133602309640192, 5, 0.0), (1560805562119557122, 19, 0.05), (1560858696292487168, 19, 0.034482758620689655), (1560903151921790976, 13, 0.0), (1560912695838253056, 18, 0.03125), (1560922829138395136, 16, 0.0), (1560930558997397504, 13, 0.0), (1560971419193905154, 19, 0.045454545454545456), (1560979203553624064, 11, 0.0), (1561003066257977346, 19, 0.07692307692307693), (1561007502577192960, 11, 0.0), (1561047276566597632, 4, 0.0), (1561048931416608770, 19, 0.045454545454545456), (1561053911863156736, 18, 0.045454545454545456), (1561058662143217664, 18, 0.03571428571428571), (1561079452783857666, 19, 0.02631578947368421), (1561081951901155328, 9, 0.0), (1561106215299534848, 17, 0.043478260869565216), (1561113481667035136, 19, 0.030303030303030304), (1561120491636752384, 8, 0.0), (1561125788639961094, 15, 0.0), (1561140051148521472, 11, 0.0), (1560826972661587968, 17, 0.02564102564102564), (1560870593582481408, 7, 0.06896551724137931), (1560888110006915074, 4, 0.0), (1560902652099264512, 8, 0.0), (1560910412895363078, 13, 0.0), (1560911768674787328, 3, 0.0), (1560927967395594242, 5, 0.0), (1560940150821191686, 7, 0.0), (1560949345486192640, 3, 0.0), (1560950248872214528, 4, 0.03571428571428571), (1560968724533743616, 9, 0.02564102564102564), (1560978505827065856, 5, 0.0), (1561028124099371010, 9, 0.0), (1561034070783676416, 3, 0.0), (1561050488413495302, 6, 0.02631578947368421), (1561055753493946368, 5, 0.0), (1561073511514472448, 8, 0.0), (1561076325242343430, 9, 0.0), (1561087172605886464, 4, 0.034482758620689655), (1561110115805270018, 18, 0.02631578947368421), (1561111272825012232, 18, 0.02631578947368421), (1561114414744576004, 5, 0.0), (1560888868244791296, 5, 0.0), (1560895053568122882, 5, 0.0), (1560926269080014848, 5, 0.0), (1560952355818012674, 8, 0.0), (1561100457375834112, 6, 0.0), (1560842373567062016, 7, 0.0), (1560888908342231040, 3, 0.0), (1560889546228879360, 3, 0.0), (1560959844735225856, 10, 0.0), (1561029894812868608, 4, 0.0), (1561087716367925250, 5, 0.0), (1560933459148902400, 4, 0.0), (1560954119413719040, 6, 0.0), (1560978579453771776, 4, 0.0), (1561093615203414016, 4, 0.0), (1560973562063818752, 4, 0.0), (1560783457080340480, 6, 0.03571428571428571), (1560998333006589954, 5, 0.0), (1560963548548894720, 6, 0.029411764705882353), (1560842711619760128, 6, 0.075), (1561113082302185472, 6, 0.03333333333333333), (1560912201904521216, 3, 0.0), (1560974642466127874, 5, 0.0), (1560985996090437634, 4, 0.0), (1560806104564654080, 3, 0.0), (1560782434572050433, 19, 0.08333333333333333), (1560784012976816133, 18, 0.0625), (1560784120271163393, 19, 0.0), (1560787664315461633, 20, 0.03225806451612903), (1560791094547386369, 20, 0.06896551724137931), (1560794834276913153, 7, 0.0), (1560796886545694723, 20, 0.029411764705882353), (1560802556246466561, 20, 0.045454545454545456), (1560823446002614273, 12, 0.0), (1560836198221766657, 20, 0.08), (1560839073685807105, 17, 0.0), (1560841247245443073, 19, 0.02564102564102564), (1560851667414917121, 8, 0.0), (1560852982970793985, 14, 0.0), (1560857557224374273, 20, 0.11538461538461539), (1560864871578755073, 20, 0.038461538461538464), (1560870960735068161, 19, 0.02564102564102564), (1560871008982241281, 20, 0.045454545454545456), (1560873095971438595, 20, 0.02702702702702703), (1560877358801887233, 13, 0.0), (1560877682312859649, 20, 0.04), (1560883839920128001, 18, 0.038461538461538464), (1560884838563041283, 4, 0.0), (1560885929438597127, 19, 0.03333333333333333), (1560887468622336003, 19, 0.034482758620689655), (1560889604487659521, 20, 0.02631578947368421), (1560897540882665477, 4, 0.0), (1560898606659502081, 20, 0.03571428571428571), (1560902874892099585, 20, 0.0), (1560909518678245381, 20, 0.030303030303030304), (1560911228238811137, 13, 0.0), (1560913367744774145, 20, 0.03225806451612903), (1560914818760167425, 20, 0.03125), (1560914913618436097, 20, 0.11764705882352941), (1560915353739448321, 20, 0.027777777777777776), (1560918519683883011, 20, 0.045454545454545456), (1560927724331466753, 4, 0.02702702702702703), (1560930089310855179, 20, 0.0), (1560931085294440455, 6, 0.0), (1560933157783760901, 10, 0.0), (1560935022680481793, 19, 0.023255813953488372), (1560937417120649217, 19, 0.027777777777777776), (1560948582160670723, 19, 0.0), (1560949458396921857, 9, 0.0), (1560950674103439361, 17, 0.0), (1560952133989662721, 20, 0.0), (1560958910303027201, 10, 0.0), (1560959494892703745, 20, 0.02857142857142857), (1560960369144864769, 6, 0.0), (1560962659536846849, 4, 0.0), (1560966326742667265, 6, 0.02564102564102564), (1560966933159329793, 4, 0.0), (1560973365745180673, 20, 0.034482758620689655), (1560975399571570689, 20, 0.023809523809523808), (1560979360609443847, 19, 0.034482758620689655), (1560983971529367555, 19, 0.043478260869565216), (1560987928838705153, 20, 0.05555555555555555), (1560990486969581569, 20, 0.047619047619047616), (1560991635118428161, 6, 0.06666666666666667), (1560991847090163721, 6, 0.025), (1560994459021725697, 18, 0.05405405405405406), (1560995224838672385, 20, 0.045454545454545456), (1560997461040177153, 20, 0.04), (1560998378003402755, 20, 0.02702702702702703), (1560999029647020033, 19, 0.03225806451612903), (1560999606715985921, 19, 0.041666666666666664), (1561003197455810561, 19, 0.05555555555555555), (1561004227241168901, 20, 0.06896551724137931), (1561007528426803203, 19, 0.08333333333333333), (1561007643984105473, 20, 0.023255813953488372), (1561012800377462785, 20, 0.08333333333333333), (1561016428840493057, 20, 0.07142857142857142), (1561019752004423681, 19, 0.05263157894736842), (1561021379666911235, 20, 0.025), (1561022853536940035, 20, 0.02857142857142857), (1561025488633618433, 20, 0.047619047619047616), (1561027784100745217, 20, 0.023255813953488372), (1561029877083607047, 20, 0.05), (1561031588527366145, 20, 0.03571428571428571), (1561032444643606529, 18, 0.030303030303030304), (1561032824257482753, 20, 0.045454545454545456), (1561044957632274433, 19, 0.027777777777777776), (1561046459331526657, 20, 0.05263157894736842), (1561047077341249543, 14, 0.0), (1561051603293061129, 20, 0.06666666666666667), (1561057432025047045, 20, 0.02564102564102564), (1561060155684536321, 20, 0.04), (1561061319117361155, 20, 0.03571428571428571), (1561063250107645953, 18, 0.02857142857142857), (1561063561299886081, 19, 0.04), (1561067847392104449, 20, 0.045454545454545456), (1561068169757745153, 17, 0.041666666666666664), (1561072978162438145, 20, 0.03125), (1561076147818995715, 20, 0.047619047619047616), (1561077662084481027, 20, 0.03333333333333333), (1561080933696110595, 18, 0.02857142857142857), (1561081008472064003, 19, 0.02857142857142857), (1561088843545206787, 20, 0.045454545454545456), (1561089486443782145, 20, 0.08), (1561110089976745985, 20, 0.041666666666666664), (1561112888118759429, 20, 0.038461538461538464), (1561113872936968195, 19, 0.047619047619047616), (1561115373801390081, 20, 0.875), (1561126108036079617, 20, 0.05555555555555555), (1561127856712732673, 20, 0.025), (1561128643895721985, 20, 0.045454545454545456), (1561133524291289089, 19, 0.027777777777777776), (1561133811919994881, 18, 0.024390243902439025), (1561137104960327685, 19, 0.027777777777777776), (1560840818604195841, 19, 0.02), (1560867744727326723, 15, 0.0), (1560869137223716865, 11, 0.0), (1560901923695370241, 18, 0.03125), (1560921086757421057, 19, 0.045454545454545456), (1560977999322824711, 4, 0.0), (1560985710605352961, 15, 0.0), (1561047015491903489, 9, 0.0), (1561072833337376769, 19, 0.023809523809523808), (1560791954908233729, 9, 0.0), (1560906059509792769, 13, 0.0), (1560943932972085249, 7, 0.06896551724137931), (1560956263558778881, 3, 0.0), (1561013202120589313, 9, 0.0), (1561032979807436801, 7, 0.0), (1561042880789041155, 9, 0.0), (1561067289591554049, 5, 0.0), (1560915553547698179, 5, 0.038461538461538464), (1560992226381029377, 11, 0.0), (1561040204584013825, 6, 0.0), (1561060133983191041, 9, 0.0), (1560846036955832321, 8, 0.0), (1560915139016429569, 8, 0.0), (1561021018717954055, 6, 0.0), (1561088683821891585, 4, 0.0)]\n",
            "=========================\n",
            "20 1000 3 100\n",
            "-------------------------\n",
            "[(1561115380600283136, 20, 1.0)]\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "search_record = en_small_twits_df[en_small_twits_df['tweetid'] == 1561115380600283136].iloc[0][['text','tweetid']]\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 100 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 100 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 200 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 200')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 10 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('10 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 25 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('25 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 1 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 1 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 3 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 3 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anH7ShijDWxh"
      },
      "source": [
        "1 - original (no FP), one have sim = 11\n",
        "\n",
        "2 - more false possitive (much more with sim 3)\n",
        "\n",
        "3 - Same as (1), but lower sim of second tweet (in (1) have 11/20, here 4/20)\n",
        "\n",
        "4 - Same as (1), but lower sim of second tweet (in (1) have 11/20, here 3/10 (~6/20))\n",
        "\n",
        "5 - Same as (1) (in (1) second tweet have 11/20, here 14/25 (~11.2/20))\n",
        "\n",
        "6 - Very strong FP results (have max sim), miss sim in (1) 11/20\n",
        "\n",
        "7 - only 100%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "f8ICYUQqDWxh",
        "outputId": "7ba81c7c-a6b0-4160-8502-33c2cbbca9a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 1000 2 100\n",
            "-------------------------\n",
            "[(1561116707489456130, 11, 0.7142857142857143), (1561126718601105410, 20, 1.0)]\n",
            "=========================\n",
            "20 100 2 100\n",
            "-------------------------\n",
            "[(1560805264298745856, 3, 0.0), (1560930740405157888, 3, 0.02857142857142857), (1561107894086406144, 3, 0.043478260869565216), (1560974837257883648, 3, 0.0625), (1561114753690476544, 3, 0.11764705882352941), (1560950910427201537, 3, 0.1111111111111111), (1560986835806986241, 3, 0.037037037037037035), (1560994714723225601, 3, 0.0), (1561035097662541825, 3, 0.04), (1561062679019769857, 3, 0.05263157894736842), (1561116707489456130, 11, 0.7142857142857143), (1561126718601105410, 20, 1.0), (1560893470889361410, 3, 0.0), (1561036089464332290, 3, 0.0), (1561090184057421834, 3, 0.043478260869565216), (1561054798224449538, 3, 0.03225806451612903), (1560964612476096515, 3, 0.0), (1561058512427524100, 3, 0.05263157894736842), (1560995446662656005, 3, 0.041666666666666664)]\n",
            "=========================\n",
            "20 1000 2 200\n",
            "-------------------------\n",
            "[(1561126718601105410, 20, 1.0), (1561116707489456130, 4, 0.7142857142857143)]\n",
            "=========================\n",
            "10 1000 2 100\n",
            "-------------------------\n",
            "[(1561126718601105410, 10, 1.0), (1561116707489456130, 3, 0.7142857142857143)]\n",
            "=========================\n",
            "25 1000 2 100\n",
            "-------------------------\n",
            "[(1561116707489456130, 14, 0.7142857142857143), (1561126718601105410, 25, 1.0)]\n",
            "=========================\n",
            "20 1000 1 100\n",
            "-------------------------\n",
            "[(1560808537755975680, 18, 0.0), (1560817312583884800, 16, 0.0), (1560833026191462400, 20, 0.0), (1560887337550241792, 20, 0.0), (1560926482020581376, 18, 0.25), (1560967450396852224, 19, 0.0), (1561020986866237440, 19, 0.045454545454545456), (1561064590426968064, 18, 0.0), (1561070652752273408, 20, 0.09090909090909091), (1561078578476945408, 19, 0.13333333333333333), (1561103880108142592, 15, 0.2), (1561104013537546240, 15, 0.2), (1561104782869381120, 17, 0.25), (1561111453658128384, 14, 0.23076923076923078), (1561116465339703296, 8, 0.15789473684210525), (1561121527881515008, 14, 0.08), (1561124076185329664, 12, 0.26666666666666666), (1561129669986074624, 7, 0.11538461538461539), (1561129707512274944, 18, 0.1111111111111111), (1561130116155117568, 13, 0.23076923076923078), (1561131833840074752, 9, 0.15789473684210525), (1561139966197092352, 15, 0.09523809523809523), (1560876786543579136, 17, 0.0), (1561106947218227200, 8, 0.10714285714285714), (1561110721018068992, 10, 0.25), (1561113725435772928, 11, 0.21428571428571427), (1561115616165117952, 11, 0.13636363636363635), (1561118812291772416, 7, 0.125), (1561119735202615296, 8, 0.15789473684210525), (1561121816596316160, 12, 0.21428571428571427), (1561124242959237120, 10, 0.10526315789473684), (1561124317789839360, 6, 0.13636363636363635), (1561135048375631872, 9, 0.11764705882352941), (1561135209243885568, 4, 0.14814814814814814), (1561106192235397120, 5, 0.10714285714285714), (1561124188622036992, 6, 0.0625), (1561124787665207296, 5, 0.03333333333333333), (1561134498284290048, 4, 0.07692307692307693), (1561106371751415808, 10, 0.125), (1561120527732834304, 4, 0.10344827586206896), (1561129587526057984, 5, 0.13043478260869565), (1560962602720804865, 11, 0.06666666666666667), (1560981819377475585, 20, 0.029411764705882353), (1561111250259656705, 14, 0.23076923076923078), (1561026186922295297, 9, 0.09523809523809523), (1561118889886781441, 8, 0.23529411764705882), (1561139980789071873, 10, 0.08), (1560933948510900225, 12, 0.041666666666666664), (1560957549012197377, 9, 0.125), (1561106909184331777, 6, 0.12), (1561113545248481290, 4, 0.0625), (1561126718601105410, 20, 1.0), (1561130718259994626, 19, 0.0), (1561111448910176258, 12, 0.3), (1561111456724262914, 13, 0.25), (1561137466064740354, 15, 0.08333333333333333), (1561138298353074178, 6, 0.08333333333333333), (1561136004236513282, 7, 0.07692307692307693), (1561135583015178242, 11, 0.15), (1561110616185737218, 4, 0.07692307692307693), (1561103922617630723, 15, 0.2), (1561109211278413827, 9, 0.06896551724137931), (1561109285815427075, 8, 0.12), (1561118152855097347, 12, 0.13333333333333333), (1561128211005702147, 7, 0.08333333333333333), (1561119013387882508, 9, 0.16666666666666666), (1561134077452894212, 11, 0.08695652173913043), (1561120255606398980, 3, 0.08695652173913043), (1560880958135570437, 20, 0.0), (1561131023613792261, 4, 0.15384615384615385), (1561103522749632518, 7, 0.07407407407407407), (1561128808257929223, 11, 0.1875)]\n",
            "=========================\n",
            "20 1000 3 100\n",
            "-------------------------\n",
            "[(1561126718601105410, 20, 1.0)]\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "search_record = en_small_twits_df[en_small_twits_df['tweetid'] == 1561126718601105410].iloc[0][['text','tweetid']]\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 100 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 100 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 200 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 200')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 10 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('10 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 25 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('25 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 1 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 1 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 3 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 3 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD3gzOWFDWxi"
      },
      "source": [
        "1 - original (no FP), Lots of same tweets, one have text repetition in it (sim = 9)\n",
        "\n",
        "2 - more false possitive (all with sim 3)\n",
        "\n",
        "3 - Same as (1), but lower sim of not max sim tweet (in (1) have 9/20, here 4/20)\n",
        "\n",
        "4 - Same as (1), but miss not max sim tweet (in (1) have 9/20)\n",
        "\n",
        "5 - Same as (1) (in (1) not max sim tweet have 9/20, here 13/25 (~10.4/20))\n",
        "\n",
        "6 - Very strong FP results (have max sim). (in (1) not max sim tweet have 9/20, here 20/20)\n",
        "\n",
        "7 - Same as (1), but lower sim of not max sim tweet (in (1) have 9/20, here 3/20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "5OfLMQFtDWxi",
        "outputId": "224c9a22-0833-4609-b23b-bc5571538a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 1000 2 100\n",
            "-------------------------\n",
            "[(1560970279572176896, 20, 1.0), (1561001346211389440, 20, 1.0), (1561001375584161800, 20, 1.0), (1561001709769572352, 20, 1.0), (1561001899930882048, 20, 1.0), (1561001313340891137, 20, 1.0), (1561001737292754945, 9, 1.0), (1561001796516155393, 20, 1.0), (1561001974484848641, 20, 1.0), (1560970918188519426, 20, 1.0), (1560971086354911235, 20, 1.0), (1561001655684198403, 20, 1.0), (1561001449408143364, 20, 1.0)]\n",
            "=========================\n",
            "20 100 2 100\n",
            "-------------------------\n",
            "[(1560970279572176896, 20, 1.0), (1561001346211389440, 20, 1.0), (1561001375584161800, 20, 1.0), (1561001709769572352, 20, 1.0), (1561001899930882048, 20, 1.0), (1561051203500494848, 3, 0.0), (1561045941808795648, 3, 0.0), (1561046919824101376, 3, 0.0), (1561001313340891137, 20, 1.0), (1561001796516155393, 20, 1.0), (1561001974484848641, 20, 1.0), (1561001737292754945, 9, 1.0), (1560797633593184257, 3, 0.0), (1561005702004592641, 3, 0.0), (1561078561158545409, 3, 0.0), (1560970918188519426, 20, 1.0), (1561119504414933002, 3, 0.0), (1561108598838759426, 3, 0.0), (1560971086354911235, 20, 1.0), (1561001655684198403, 20, 1.0), (1561109285815427075, 3, 0.0), (1561001449408143364, 20, 1.0), (1561075918256357388, 3, 0.0), (1561088709222699013, 3, 0.0), (1560990941019783175, 3, 0.03571428571428571)]\n",
            "=========================\n",
            "20 1000 2 200\n",
            "-------------------------\n",
            "[(1560970279572176896, 20, 1.0), (1561001346211389440, 20, 1.0), (1561001375584161800, 20, 1.0), (1561001709769572352, 20, 1.0), (1561001899930882048, 20, 1.0), (1561001313340891137, 20, 1.0), (1561001796516155393, 20, 1.0), (1561001974484848641, 20, 1.0), (1561001737292754945, 4, 1.0), (1560970918188519426, 20, 1.0), (1560971086354911235, 20, 1.0), (1561001655684198403, 20, 1.0), (1561001449408143364, 20, 1.0)]\n",
            "=========================\n",
            "10 1000 2 100\n",
            "-------------------------\n",
            "[(1560970279572176896, 10, 1.0), (1561001346211389440, 10, 1.0), (1561001375584161800, 10, 1.0), (1561001709769572352, 10, 1.0), (1561001899930882048, 10, 1.0), (1561001313340891137, 10, 1.0), (1561001796516155393, 10, 1.0), (1561001974484848641, 10, 1.0), (1560970918188519426, 10, 1.0), (1560971086354911235, 10, 1.0), (1561001655684198403, 10, 1.0), (1561001449408143364, 10, 1.0)]\n",
            "=========================\n",
            "25 1000 2 100\n",
            "-------------------------\n",
            "[(1560970279572176896, 25, 1.0), (1561001346211389440, 25, 1.0), (1561001375584161800, 25, 1.0), (1561001709769572352, 25, 1.0), (1561001899930882048, 25, 1.0), (1561001313340891137, 25, 1.0), (1561001796516155393, 25, 1.0), (1561001974484848641, 25, 1.0), (1561001737292754945, 13, 1.0), (1560970918188519426, 25, 1.0), (1560971086354911235, 25, 1.0), (1561001655684198403, 25, 1.0), (1561001449408143364, 25, 1.0)]\n",
            "=========================\n",
            "20 1000 1 100\n",
            "-------------------------\n",
            "[(1560786255847329792, 5, 0.13333333333333333), (1560844815168057344, 10, 0.0625), (1560858515048394752, 4, 0.045454545454545456), (1560895676367736832, 9, 0.05263157894736842), (1560901920750985216, 12, 0.09090909090909091), (1560905672300036096, 9, 0.09090909090909091), (1560925479279988736, 16, 0.14285714285714285), (1560938106601279488, 16, 0.14285714285714285), (1560943784975794176, 8, 0.04), (1560970279572176896, 20, 1.0), (1560990210745253888, 11, 0.07142857142857142), (1560993789988245504, 12, 0.045454545454545456), (1561001346211389440, 20, 1.0), (1561001375584161800, 20, 1.0), (1561001709769572352, 20, 1.0), (1561001899930882048, 20, 1.0), (1561025372589793280, 7, 0.05263157894736842), (1561076139879288832, 12, 0.13333333333333333), (1561091244188655616, 12, 0.09090909090909091), (1561107068743974912, 11, 0.2), (1561111520360235008, 16, 0.16666666666666666), (1561119352581017600, 12, 0.15384615384615385), (1561119746682232832, 13, 0.1), (1561121384754987008, 13, 0.09090909090909091), (1561123748140310528, 13, 0.0625), (1561137266281570304, 13, 0.047619047619047616), (1560913263000444928, 10, 0.09090909090909091), (1560935850321534976, 9, 0.23076923076923078), (1561040459639754752, 12, 0.1111111111111111), (1561113284115406848, 13, 0.07142857142857142), (1561133332376817664, 9, 0.05263157894736842), (1561053311968714752, 7, 0.05), (1560903776575180800, 3, 0.041666666666666664), (1560981161593262080, 5, 0.0625), (1561106588919889920, 6, 0.1), (1561107830215774208, 6, 0.07692307692307693), (1560859224225226753, 7, 0.09523809523809523), (1560891870749900801, 8, 0.18181818181818182), (1560900142106124289, 8, 0.038461538461538464), (1560900250008596481, 12, 0.07142857142857142), (1560905532801785857, 13, 0.2222222222222222), (1560913631004372993, 12, 0.05555555555555555), (1560922690025918465, 12, 0.06666666666666667), (1560960845877739521, 10, 0.03125), (1561001313340891137, 20, 1.0), (1561001737292754945, 20, 1.0), (1561001796516155393, 20, 1.0), (1561001974484848641, 20, 1.0), (1561033076779671553, 10, 0.0625), (1561049393532264449, 8, 0.0625), (1561087012341485577, 10, 0.05263157894736842), (1561122500557275137, 16, 0.08333333333333333), (1561125764917075977, 14, 0.07142857142857142), (1561138043142242305, 12, 0.041666666666666664), (1560912819935125505, 5, 0.04), (1560960350681550849, 9, 0.05555555555555555), (1561132429678903297, 4, 0.04), (1560944138857549825, 4, 0.03333333333333333), (1560930354072064002, 6, 0.043478260869565216), (1560949264011800578, 9, 0.043478260869565216), (1560970918188519426, 20, 1.0), (1561021282921111554, 14, 0.125), (1561059256119558146, 8, 0.07692307692307693), (1560805774812667906, 5, 0.05555555555555555), (1560928589159202818, 5, 0.11764705882352941), (1560956191307710466, 6, 0.05263157894736842), (1560971086354911235, 20, 1.0), (1561001655684198403, 20, 1.0), (1561039068926025731, 5, 0.041666666666666664), (1561121659691667459, 16, 0.2857142857142857), (1561133452124094467, 8, 0.06666666666666667), (1561046107655966723, 6, 0.03333333333333333), (1560967977918517252, 10, 0.1), (1561001449408143364, 20, 1.0), (1561058047379767308, 8, 0.08333333333333333), (1561118367003582468, 4, 0.14285714285714285), (1560908658363473925, 10, 0.038461538461538464), (1561127726999900165, 10, 0.058823529411764705), (1561119955684319239, 10, 0.043478260869565216)]\n",
            "=========================\n",
            "20 1000 3 100\n",
            "-------------------------\n",
            "[(1560970279572176896, 20, 1.0), (1561001346211389440, 20, 1.0), (1561001375584161800, 20, 1.0), (1561001709769572352, 20, 1.0), (1561001899930882048, 20, 1.0), (1561001313340891137, 20, 1.0), (1561001796516155393, 20, 1.0), (1561001974484848641, 20, 1.0), (1561001737292754945, 3, 1.0), (1560970918188519426, 20, 1.0), (1560971086354911235, 20, 1.0), (1561001655684198403, 20, 1.0), (1561001449408143364, 20, 1.0)]\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "search_record = en_small_twits_df[en_small_twits_df['tweetid'] == 1560970279572176896].iloc[0][['text','tweetid']]\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 100 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 100 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 200 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 200')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 10 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('10 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 25 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('25 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 1 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 1 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 3 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 3 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5WxqnqyDWxj"
      },
      "source": [
        "1 - original (no FP). only target tweet\n",
        "\n",
        "2 - lot false possitive (sim = 3/4)\n",
        "\n",
        "3 - only target tweet\n",
        "\n",
        "4 - only target tweet\n",
        "\n",
        "5 - only target tweet\n",
        "\n",
        "6 - Have lots of similar tweets (they have same key words as target tweet: Pantsir-S1 (can be writed as Pancyr-S1), Sewastopol), Some ammount of FP\n",
        "\n",
        "7 - only target tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "4nKGBfPeDWxj",
        "outputId": "425f5e53-5409-4a68-fe0f-2f5fc0ac8db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 1000 2 100\n",
            "-------------------------\n",
            "[(1560916346828996608, 20, 1.0)]\n",
            "=========================\n",
            "20 100 2 100\n",
            "-------------------------\n",
            "[(1560886162759172096, 3, 0.0), (1560916346828996608, 20, 1.0), (1560937162677268480, 3, 0.03125), (1560827506978164736, 3, 0.10526315789473684), (1560945937782296576, 3, 0.043478260869565216), (1560976127992135680, 3, 0.02564102564102564), (1560989011027664896, 3, 0.041666666666666664), (1560851342498750464, 3, 0.0), (1560908287318659072, 3, 0.0), (1561070793689145344, 3, 0.038461538461538464), (1560919679476047872, 3, 0.04), (1560909244425281537, 3, 0.0), (1560918325894737921, 3, 0.06451612903225806), (1561029584115666945, 4, 0.041666666666666664), (1560926733171408897, 3, 0.07692307692307693), (1561018683828445185, 3, 0.0), (1561050438052392961, 3, 0.030303030303030304), (1561001522317729793, 3, 0.0), (1560826958396768257, 3, 0.0), (1560914536793874433, 3, 0.034482758620689655), (1560860588875534337, 3, 0.05), (1560970296131309570, 3, 0.0), (1560987779307376642, 3, 0.047619047619047616), (1561112530793205763, 3, 0.0), (1561108821162033155, 3, 0.03125), (1561115038097854467, 3, 0.0), (1561078033573879812, 3, 0.0)]\n",
            "=========================\n",
            "20 1000 2 200\n",
            "-------------------------\n",
            "[(1560916346828996608, 20, 1.0)]\n",
            "=========================\n",
            "10 1000 2 100\n",
            "-------------------------\n",
            "[(1560916346828996608, 10, 1.0)]\n",
            "=========================\n",
            "25 1000 2 100\n",
            "-------------------------\n",
            "[(1560916346828996608, 25, 1.0)]\n",
            "=========================\n",
            "20 1000 1 100\n",
            "-------------------------\n",
            "[(1560916346828996608, 20, 1.0), (1561057281386717184, 20, 0.1), (1561060211246391296, 19, 0.25), (1561061437648343040, 18, 0.35), (1561062959861977088, 18, 0.23809523809523808), (1561094724341051392, 20, 0.11538461538461539), (1561058442504278017, 19, 0.35294117647058826), (1561059158694166529, 19, 0.35294117647058826), (1561061598575353857, 20, 0.0), (1561063218130321409, 20, 0.11538461538461539), (1561079629057851393, 19, 0.18518518518518517), (1560923284740444162, 8, 0.038461538461538464), (1561020020666384387, 18, 0.0), (1561059381088763907, 19, 0.35294117647058826), (1561064029568864259, 20, 0.14285714285714285), (1561086669289299979, 20, 0.15384615384615385), (1561056148278714374, 20, 0.2857142857142857), (1561131476774780935, 19, 0.25)]\n",
            "=========================\n",
            "20 1000 3 100\n",
            "-------------------------\n",
            "[(1560916346828996608, 20, 1.0)]\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "search_record = en_small_twits_df[en_small_twits_df['tweetid'] == 1560916346828996608].iloc[0][['text','tweetid']]\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 100 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 100 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 200 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 200')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 10 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('10 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 25 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('25 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 1 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 1 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 3 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 3 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aUCx2IQDWxj"
      },
      "source": [
        "1 - All tweets have same text, but different '@user'. All of them have sim 3-6 (exept target that have 20), have some ammount of false negative (see (2),(5), have 9 tweets)\n",
        "\n",
        "2 - lot false possitive (sim = 3), found some other similar tweets\n",
        "\n",
        "3 - only target tweet\n",
        "\n",
        "4 - only target tweet\n",
        "\n",
        "5 - Same as (1), but have more tweets (14 tweets)\n",
        "\n",
        "6 - Have lots of similar tweets (same text, have sim > 10 ), Large ammount of FP, some ammount of FP with sim > 10. total results 294, 17 have same text with sim > 10\n",
        "\n",
        "7 - Same as (1), but have less tweets (6 tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "dWhlPpyeDWxj",
        "outputId": "bbae415d-79d8-4715-cc25-d91476f26e73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 1000 2 100\n",
            "-------------------------\n",
            "[(1560872710565433344, 20, 1.0), (1560871349652193280, 6, 0.7142857142857143), (1560871537062080512, 6, 0.7142857142857143), (1560871622214828032, 4, 0.7142857142857143), (1560872209807470592, 5, 0.7142857142857143), (1560872515630960640, 4, 0.7142857142857143), (1560872980800208896, 3, 0.7142857142857143), (1560871041219837952, 3, 0.7142857142857143), (1560872600980824066, 4, 0.7142857142857143)]\n",
            "=========================\n",
            "20 100 2 100\n",
            "-------------------------\n",
            "[(1560872710565433344, 20, 1.0), (1560916833129021440, 3, 0.0), (1561124380536885248, 3, 0.0), (1560888073055084544, 3, 0.0), (1560950217440202752, 3, 0.0), (1560983930781609984, 3, 0.05555555555555555), (1560871622214828032, 4, 0.7142857142857143), (1560872209807470592, 5, 0.7142857142857143), (1560963458065174528, 3, 0.0), (1560864888049860608, 3, 0.0), (1560871349652193280, 6, 0.7142857142857143), (1560871537062080512, 6, 0.7142857142857143), (1560872515630960640, 4, 0.7142857142857143), (1560872980800208896, 3, 0.7142857142857143), (1560878548038057984, 3, 0.0), (1560976330706935808, 3, 0.0), (1561099278252138496, 3, 0.0), (1560871896933335040, 3, 0.625), (1560871041219837952, 3, 0.7142857142857143), (1560999975806332928, 3, 0.0), (1561098746695503873, 3, 0.0), (1560915097240932353, 3, 0.041666666666666664), (1560875941240442881, 3, 0.0), (1560899901361426433, 3, 0.0), (1560872600980824066, 4, 0.7142857142857143), (1560904480400474114, 3, 0.038461538461538464), (1560956344106115074, 3, 0.0), (1561126506671267842, 3, 0.0), (1560828229274968066, 3, 0.05), (1561139504810844163, 3, 0.0), (1560997622344601603, 3, 0.0), (1561098519691382789, 3, 0.0)]\n",
            "=========================\n",
            "20 1000 2 200\n",
            "-------------------------\n",
            "[(1560872710565433344, 20, 1.0)]\n",
            "=========================\n",
            "10 1000 2 100\n",
            "-------------------------\n",
            "[(1560872710565433344, 10, 1.0)]\n",
            "=========================\n",
            "25 1000 2 100\n",
            "-------------------------\n",
            "[(1560871041219837952, 4, 0.7142857142857143), (1560871349652193280, 6, 0.7142857142857143), (1560871537062080512, 6, 0.7142857142857143), (1560871622214828032, 4, 0.7142857142857143), (1560871796886614016, 3, 0.7142857142857143), (1560872710565433344, 25, 1.0), (1560872980800208896, 5, 0.7142857142857143), (1560871160803651584, 3, 0.7142857142857143), (1560872002495254528, 3, 0.7142857142857143), (1560872209807470592, 6, 0.7142857142857143), (1560872515630960640, 4, 0.7142857142857143), (1560872783646969856, 3, 0.7142857142857143), (1560872600980824066, 5, 0.7142857142857143), (1560872872746561542, 3, 0.7142857142857143)]\n",
            "=========================\n",
            "20 1000 1 100\n",
            "-------------------------\n",
            "[(1560872710565433344, 20, 1.0), (1560779750062362624, 3, 0.05263157894736842), (1560782468021899264, 6, 0.03333333333333333), (1560792126375755776, 9, 0.08333333333333333), (1560799033098063872, 7, 0.058823529411764705), (1560812413636730880, 7, 0.037037037037037035), (1560824950704988160, 3, 0.05555555555555555), (1560837355732353024, 7, 0.0), (1560868143207161856, 6, 0.06666666666666667), (1560871041219837952, 16, 0.7142857142857143), (1560871160803651584, 18, 0.7142857142857143), (1560871273252925440, 18, 0.7142857142857143), (1560871622214828032, 16, 0.7142857142857143), (1560872080362831872, 13, 0.5555555555555556), (1560872126021869568, 6, 0.043478260869565216), (1560872209807470592, 15, 0.7142857142857143), (1560872314623131648, 16, 0.7142857142857143), (1560872515630960640, 16, 0.7142857142857143), (1560872783646969856, 18, 0.7142857142857143), (1560872980800208896, 16, 0.7142857142857143), (1560875380785168384, 6, 0.058823529411764705), (1560878722810527744, 8, 0.07142857142857142), (1560880588650778624, 7, 0.047619047619047616), (1560884713065025536, 8, 0.06666666666666667), (1560886672283377664, 9, 0.0), (1560887376611778560, 4, 0.07142857142857142), (1560887832390127616, 9, 0.06666666666666667), (1560898191226183680, 4, 0.05555555555555555), (1560902958774099968, 9, 0.058823529411764705), (1560904881451532288, 7, 0.07692307692307693), (1560905024745750528, 7, 0.07692307692307693), (1560907205116940288, 4, 0.07692307692307693), (1560911031748022272, 12, 0.14285714285714285), (1560919795633389568, 7, 0.06666666666666667), (1560922956012093440, 8, 0.07692307692307693), (1560923336263319552, 9, 0.0625), (1560933665084948480, 3, 0.047619047619047616), (1560941001644118016, 8, 0.1), (1560941549961093120, 3, 0.05555555555555555), (1560942688551473152, 8, 0.09090909090909091), (1560945265565224960, 5, 0.04), (1560953097601662976, 9, 0.1), (1560965094011461632, 8, 0.058823529411764705), (1560968816540229632, 5, 0.07692307692307693), (1560969405584080896, 7, 0.0), (1560969944481013760, 5, 0.08333333333333333), (1560978592959545344, 3, 0.047619047619047616), (1560995131343388672, 5, 0.058823529411764705), (1560997230701449216, 4, 0.08333333333333333), (1560998246549716992, 5, 0.0625), (1561005232406020096, 9, 0.07692307692307693), (1561007100284882944, 7, 0.07142857142857142), (1561013007081160704, 9, 0.07142857142857142), (1561013215781195776, 9, 0.08333333333333333), (1561018848916246528, 4, 0.03125), (1561018877009592320, 7, 0.07692307692307693), (1561019448840007680, 8, 0.06666666666666667), (1561019993080401920, 7, 0.09090909090909091), (1561024184968515584, 6, 0.08333333333333333), (1561028077777342464, 3, 0.045454545454545456), (1561030762673197056, 9, 0.07142857142857142), (1561041330985377792, 9, 0.05555555555555555), (1561050113954512896, 4, 0.041666666666666664), (1561053072142499840, 7, 0.07142857142857142), (1561056880016756736, 5, 0.07692307692307693), (1561059624501084160, 5, 0.09090909090909091), (1561063730972184576, 4, 0.041666666666666664), (1561065986266202112, 6, 0.07692307692307693), (1561068349030817792, 9, 0.1), (1561074003124654080, 9, 0.03571428571428571), (1561075876577497088, 4, 0.09090909090909091), (1561085229665771520, 7, 0.07142857142857142), (1561088786657816576, 8, 0.06666666666666667), (1561090233810255872, 6, 0.0625), (1561090728540999680, 9, 0.1111111111111111), (1561103928673980416, 5, 0.0625), (1561104276776255488, 6, 0.07142857142857142), (1561104632679919616, 9, 0.1111111111111111), (1561113184349683712, 7, 0.07692307692307693), (1561116943419346944, 3, 0.07142857142857142), (1561117402284400640, 3, 0.043478260869565216), (1561123047230390272, 9, 0.125), (1561129650847358976, 7, 0.07692307692307693), (1561132583227998208, 7, 0.058823529411764705), (1560800552119844864, 5, 0.045454545454545456), (1560808398698192896, 4, 0.045454545454545456), (1560818457079332864, 4, 0.03333333333333333), (1560818905005887488, 3, 0.043478260869565216), (1560827980880007168, 4, 0.06666666666666667), (1560838311924576256, 4, 0.0), (1560871796886614016, 14, 0.7142857142857143), (1560872002495254528, 12, 0.7142857142857143), (1560892415736684544, 8, 0.1), (1560906987725996032, 6, 0.0625), (1560908160893861888, 5, 0.0625), (1560916816330989568, 5, 0.038461538461538464), (1560933497602285568, 6, 0.05555555555555555), (1560989011027664896, 6, 0.05555555555555555), (1560990551612297216, 3, 0.045454545454545456), (1561005350257573888, 6, 0.07142857142857142), (1561040465276829696, 4, 0.09090909090909091), (1561064857411125248, 4, 0.043478260869565216), (1561066899156488192, 4, 0.0), (1561091278707871744, 6, 0.05555555555555555), (1561096523093397504, 7, 0.058823529411764705), (1561123791425740800, 4, 0.04), (1561127513018863616, 7, 0.1111111111111111), (1561129602235392000, 5, 0.07692307692307693), (1560871714116218880, 9, 0.5), (1560797850409340928, 4, 0.08333333333333333), (1560802779442077696, 4, 0.038461538461538464), (1560812427318525952, 4, 0.05263157894736842), (1560842812530245632, 3, 0.05263157894736842), (1560864505835429888, 4, 0.05555555555555555), (1560884804073185280, 5, 0.07692307692307693), (1560890438638354432, 4, 0.0625), (1560909086245490688, 4, 0.05263157894736842), (1560913406449917952, 3, 0.037037037037037035), (1560913553644605440, 5, 0.03571428571428571), (1560951416453201920, 3, 0.05263157894736842), (1560953290183114752, 3, 0.09090909090909091), (1561020810298531840, 3, 0.07142857142857142), (1561023332719308800, 4, 0.07692307692307693), (1561102850956361728, 3, 0.05), (1561110840102862848, 5, 0.058823529411764705), (1560811888316923904, 4, 0.05263157894736842), (1560918386372132864, 5, 0.0), (1560918582820851712, 3, 0.058823529411764705), (1560973372426797056, 5, 0.047619047619047616), (1561035599640875008, 5, 0.09090909090909091), (1561048591812239360, 3, 0.037037037037037035), (1561064551361269760, 3, 0.047619047619047616), (1561121873219371008, 5, 0.05263157894736842), (1561127926296354816, 4, 0.041666666666666664), (1560999877420908544, 4, 0.08333333333333333), (1561030581147901952, 3, 0.047619047619047616), (1561101751398240256, 3, 0.07142857142857142), (1560969365398458368, 3, 0.04), (1561093961447624704, 3, 0.0), (1560783169640595457, 8, 0.09090909090909091), (1560785757899456513, 4, 0.047619047619047616), (1560824323161481217, 7, 0.1111111111111111), (1560853125820551169, 6, 0.0), (1560862746450821121, 3, 0.058823529411764705), (1560865417337475073, 4, 0.058823529411764705), (1560867089174380545, 6, 0.09090909090909091), (1560875233405452289, 7, 0.1111111111111111), (1560881755439206401, 9, 0.09090909090909091), (1560884480579100673, 4, 0.05555555555555555), (1560886621146423297, 8, 0.06666666666666667), (1560899452990361601, 4, 0.09090909090909091), (1560904991379972097, 7, 0.07692307692307693), (1560907806429134849, 4, 0.058823529411764705), (1560908807152119809, 5, 0.09090909090909091), (1560914682709479425, 6, 0.0), (1560915097240932353, 5, 0.041666666666666664), (1560922035999703041, 8, 0.07142857142857142), (1560926079631728641, 6, 0.0625), (1560927218859114497, 9, 0.07142857142857142), (1560929011047563265, 7, 0.05263157894736842), (1560929902077087745, 3, 0.034482758620689655), (1560934667821490177, 3, 0.0625), (1560935199512354817, 8, 0.0), (1560940759485976577, 7, 0.1), (1560941355471339521, 4, 0.05555555555555555), (1560943522152341505, 8, 0.08333333333333333), (1560949626580205569, 9, 0.07142857142857142), (1560951639565008897, 4, 0.07142857142857142), (1560958907010400257, 8, 0.058823529411764705), (1560974606030225409, 7, 0.09090909090909091), (1560977018317070337, 6, 0.08333333333333333), (1560979281781694465, 8, 0.0625), (1560981483950587905, 4, 0.047619047619047616), (1560982772319125505, 7, 0.08333333333333333), (1560985839294787585, 6, 0.06666666666666667), (1561016053538312193, 5, 0.0), (1561025917002174465, 7, 0.1), (1561036623265124353, 5, 0.08333333333333333), (1561048041917931521, 9, 0.125), (1561050643917258753, 9, 0.047619047619047616), (1561053147266793473, 5, 0.05555555555555555), (1561054697867419649, 6, 0.06666666666666667), (1561056996190740481, 7, 0.08333333333333333), (1561081256648982529, 5, 0.05263157894736842), (1561090034798821377, 4, 0.038461538461538464), (1561117161535553537, 5, 0.05263157894736842), (1561132531939942401, 9, 0.125), (1560827461419409409, 5, 0.058823529411764705), (1560848703736279041, 3, 0.047619047619047616), (1560854412825878529, 4, 0.07692307692307693), (1560861850085994497, 3, 0.07142857142857142), (1560918980315197441, 3, 0.0625), (1560920928908976129, 5, 0.05555555555555555), (1560955715556106241, 3, 0.06666666666666667), (1560957393428746241, 4, 0.038461538461538464), (1560965404209680385, 5, 0.047619047619047616), (1560974930757525505, 4, 0.05555555555555555), (1560978456388706305, 3, 0.06666666666666667), (1560983255305838593, 4, 0.03333333333333333), (1560984965243981825, 3, 0.06666666666666667), (1560998415894560769, 3, 0.058823529411764705), (1561009658168594433, 6, 0.09090909090909091), (1561009808752476161, 3, 0.05263157894736842), (1561020898034974721, 4, 0.05263157894736842), (1561042753777287169, 4, 0.08333333333333333), (1561089780695404545, 3, 0.043478260869565216), (1561122270365499401, 5, 0.0), (1560844948475371521, 4, 0.058823529411764705), (1560866238586392577, 5, 0.06666666666666667), (1560938759654461441, 5, 0.047619047619047616), (1560948688293347329, 3, 0.0625), (1560966700413210625, 6, 0.047619047619047616), (1560976683846483969, 4, 0.0), (1561052314206875649, 4, 0.07142857142857142), (1561116253959245825, 9, 0.14285714285714285), (1561125620976738305, 5, 0.05555555555555555), (1561133884623884289, 3, 0.04), (1561136587295129601, 3, 0.045454545454545456), (1561009528765816833, 3, 0.058823529411764705), (1561046147476574209, 3, 0.03571428571428571), (1561084080166506497, 5, 0.06666666666666667), (1561134516290330625, 4, 0.058823529411764705), (1560974937522950145, 3, 0.041666666666666664), (1560782643301580802, 7, 0.06666666666666667), (1560804820017086466, 7, 0.05555555555555555), (1560840911076134914, 4, 0.09090909090909091), (1560855935832268802, 5, 0.06666666666666667), (1560856835002884098, 8, 0.1111111111111111), (1560862734455083010, 5, 0.08333333333333333), (1560871438797934594, 13, 0.625), (1560872409183715330, 16, 0.7142857142857143), (1560872600980824066, 18, 0.7142857142857143), (1560872664771792898, 5, 0.08333333333333333), (1560879052155748354, 5, 0.045454545454545456), (1560896978153766914, 6, 0.06666666666666667), (1560898540536098818, 9, 0.07692307692307693), (1560911478823309314, 9, 0.04), (1560917301594968066, 9, 0.05), (1560935603121917954, 9, 0.1111111111111111), (1560991478687596546, 5, 0.06666666666666667), (1561036345312743426, 8, 0.06666666666666667), (1561058407112744962, 7, 0.058823529411764705), (1561079910680170498, 7, 0.0625), (1561090427071217666, 9, 0.06666666666666667), (1561093272428322818, 6, 0.07692307692307693), (1561101909733150722, 6, 0.09090909090909091), (1560890904088612866, 4, 0.058823529411764705), (1560891563563180034, 3, 0.038461538461538464), (1560922836323147778, 8, 0.09090909090909091), (1560932223980822530, 6, 0.06666666666666667), (1560945924121264130, 5, 0.038461538461538464), (1560946966191898626, 3, 0.047619047619047616), (1560968413878558722, 5, 0.058823529411764705), (1561060690206629890, 4, 0.038461538461538464), (1561062186549710850, 4, 0.041666666666666664), (1561128099957424130, 4, 0.1111111111111111), (1561138336026300418, 4, 0.037037037037037035), (1560779357198614530, 3, 0.07142857142857142), (1561043624615354370, 4, 0.07692307692307693), (1560955895554756610, 3, 0.05263157894736842), (1560956121266913282, 3, 0.05), (1560971118093410307, 8, 0.0625), (1561002927950422019, 6, 0.037037037037037035), (1561010581473312771, 8, 0.08333333333333333), (1561014853879332867, 6, 0.07692307692307693), (1561052448990560259, 8, 0.09090909090909091), (1561129647160664067, 7, 0.05263157894736842), (1561077675615191043, 5, 0.07692307692307693), (1561042069950439427, 3, 0.03333333333333333), (1561125902448295939, 3, 0.047619047619047616), (1561134988501942275, 5, 0.05555555555555555), (1560805215716233219, 3, 0.08333333333333333), (1561133551684407299, 3, 0.045454545454545456), (1560876809230721028, 6, 0.0), (1561043269919834116, 6, 0.05263157894736842), (1560782306989772804, 5, 0.125), (1560936692680458244, 5, 0.058823529411764705), (1561114308125380612, 5, 0.125), (1561078252222963716, 3, 0.037037037037037035), (1561023754918154245, 5, 0.08333333333333333), (1561114356259119109, 4, 0.05555555555555555), (1560967274999185413, 7, 0.09090909090909091), (1560802733883531269, 4, 0.038461538461538464), (1561000181780418565, 4, 0.045454545454545456), (1560813677804224518, 7, 0.05555555555555555), (1560872872746561542, 18, 0.7142857142857143), (1560941026709123078, 3, 0.038461538461538464), (1560916742653739014, 5, 0.038461538461538464), (1561120936895684614, 4, 0.07142857142857142), (1561088220112330759, 4, 0.05555555555555555), (1561089799674630151, 3, 0.05555555555555555), (1561105748100354055, 7, 0.1), (1560949505167695879, 3, 0.043478260869565216), (1560982429887766535, 5, 0.03571428571428571)]\n",
            "=========================\n",
            "20 1000 3 100\n",
            "-------------------------\n",
            "[(1560872710565433344, 20, 1.0), (1560871622214828032, 4, 0.7142857142857143), (1560871796886614016, 5, 0.7142857142857143), (1560872209807470592, 3, 0.7142857142857143), (1560872409183715330, 3, 0.7142857142857143), (1560872600980824066, 3, 0.7142857142857143)]\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "search_record = en_small_twits_df[en_small_twits_df['tweetid'] == 1560872710565433344].iloc[0][['text','tweetid']]\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 100 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 100 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 200 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 200')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 10 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('10 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 25 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('25 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 1 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 1 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 3 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 3 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuZKt-CpDWxk"
      },
      "source": [
        "1 - only target tweet\n",
        "\n",
        "2 - false possitive (sim = 3)\n",
        "\n",
        "3 - only target tweet\n",
        "\n",
        "4 - only target tweet\n",
        "\n",
        "5 - only target tweet\n",
        "\n",
        "6 - Have lots of FP with max or close simtotal results 520\n",
        "\n",
        "7 - Have target tweet + 1 FP tweet with sim = 3 (ony folowing strings have something simmilar \"from #Russia's hard\" and \"from #Russia had\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "gLVeBni6DWxk",
        "outputId": "d6473509-272c-4ff1-9b85-4301c61e060f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 1000 2 100\n",
            "-------------------------\n",
            "[(1561027561022423043, 20, 1.0)]\n",
            "=========================\n",
            "20 100 2 100\n",
            "-------------------------\n",
            "[(1560779589487583232, 3, 0.0), (1560858696292487168, 3, 0.0), (1560983713713754112, 3, 0.03571428571428571), (1561111095397556224, 3, 0.0), (1561121370775379968, 3, 0.0), (1560983606100725760, 3, 0.0), (1561031847643054080, 3, 0.0), (1560842666920669185, 3, 0.0), (1561057848884432898, 3, 0.0), (1561027561022423043, 20, 1.0)]\n",
            "=========================\n",
            "20 1000 2 200\n",
            "-------------------------\n",
            "[(1561027561022423043, 20, 1.0)]\n",
            "=========================\n",
            "10 1000 2 100\n",
            "-------------------------\n",
            "[(1561027561022423043, 10, 1.0)]\n",
            "=========================\n",
            "25 1000 2 100\n",
            "-------------------------\n",
            "[(1561027561022423043, 25, 1.0)]\n",
            "=========================\n",
            "20 1000 1 100\n",
            "-------------------------\n",
            "[(1560778754300968960, 20, 0.04081632653061224), (1560782087908884480, 20, 0.023255813953488372), (1560790436801028096, 20, 0.058823529411764705), (1560792608493953024, 20, 0.047619047619047616), (1560793086229446656, 20, 0.022222222222222223), (1560802368119136256, 20, 0.02702702702702703), (1560803703984033792, 20, 0.021739130434782608), (1560805264298745856, 20, 0.02040816326530612), (1560806853621166080, 20, 0.027777777777777776), (1560813476603478016, 20, 0.06976744186046512), (1560816112693956608, 20, 0.05714285714285714), (1560816529582833664, 20, 0.04081632653061224), (1560820165935259648, 20, 0.0196078431372549), (1560820456453701632, 20, 0.07894736842105263), (1560823954041970688, 20, 0.02127659574468085), (1560824204659924992, 20, 0.038461538461538464), (1560824428900061184, 20, 0.06382978723404255), (1560826946078089216, 20, 0.0625), (1560833854369415168, 19, 0.0), (1560835221532073984, 20, 0.022727272727272728), (1560836389461172224, 20, 0.0625), (1560838340743573504, 20, 0.037037037037037035), (1560838743917219840, 6, 0.02631578947368421), (1560841179192762368, 20, 0.018867924528301886), (1560841403332235264, 20, 0.05714285714285714), (1560841639840694272, 20, 0.02564102564102564), (1560842181371633664, 20, 0.0196078431372549), (1560842307410149376, 20, 0.07894736842105263), (1560848464015130624, 20, 0.046511627906976744), (1560852099713351680, 20, 0.02564102564102564), (1560853510140219392, 20, 0.04878048780487805), (1560857300390346752, 20, 0.06666666666666667), (1560859975693774848, 20, 0.024390243902439025), (1560860089048879104, 20, 0.022222222222222223), (1560863045252853760, 20, 0.0), (1560863473436971008, 20, 0.05714285714285714), (1560863509147070464, 19, 0.0), (1560865230762262528, 20, 0.022222222222222223), (1560870501232304128, 20, 0.0392156862745098), (1560871349652193280, 20, 0.0), (1560871537062080512, 20, 0.0), (1560874385543778304, 20, 0.019230769230769232), (1560878862891900928, 20, 0.01694915254237288), (1560882273565786112, 20, 0.04), (1560884969525886976, 20, 0.046511627906976744), (1560885617327734784, 20, 0.02127659574468085), (1560887707294908416, 20, 0.043478260869565216), (1560892298497384448, 20, 0.02702702702702703), (1560894149846605824, 20, 0.022727272727272728), (1560895469907312640, 20, 0.02127659574468085), (1560897299450060800, 20, 0.058823529411764705), (1560898283245027328, 20, 0.018867924528301886), (1560899696448733184, 20, 0.0425531914893617), (1560900031011360768, 20, 0.02631578947368421), (1560904466135715840, 20, 0.02564102564102564), (1560904907284066304, 20, 0.044444444444444446), (1560905419282944000, 20, 0.0196078431372549), (1560906017629683712, 20, 0.07142857142857142), (1560906515111985152, 20, 0.04), (1560906784960925696, 20, 0.04878048780487805), (1560909164012093440, 20, 0.038461538461538464), (1560909710555074560, 20, 0.02564102564102564), (1560910636586094592, 20, 0.02040816326530612), (1560911633983193088, 20, 0.018518518518518517), (1560913808742293504, 20, 0.017241379310344827), (1560915021965688832, 20, 0.08333333333333333), (1560915285279997952, 20, 0.08571428571428572), (1560915869135474688, 20, 0.08333333333333333), (1560917016906694656, 20, 0.025), (1560918058348253184, 20, 0.05263157894736842), (1560918785615331328, 20, 0.023809523809523808), (1560919568704409600, 20, 0.023255813953488372), (1560920374166142976, 20, 0.020833333333333332), (1560920401286467584, 20, 0.020833333333333332), (1560920964317290496, 19, 0.0), (1560921053681131520, 20, 0.02631578947368421), (1560921060186497024, 20, 0.022727272727272728), (1560921771846647808, 20, 0.08823529411764706), (1560922542197571584, 20, 0.02127659574468085), (1560925957262716928, 20, 0.029411764705882353), (1560926465302183936, 20, 0.014925373134328358), (1560926615504404480, 20, 0.02564102564102564), (1560929672954712064, 20, 0.05555555555555555), (1560930095539396608, 20, 0.021739130434782608), (1560932998798876672, 20, 0.03571428571428571), (1560935158580224000, 20, 0.022222222222222223), (1560936525499715584, 20, 0.034482758620689655), (1560936932053766144, 20, 0.05714285714285714), (1560937638181638144, 20, 0.05714285714285714), (1560938645657624576, 20, 0.0196078431372549), (1560940585573576704, 20, 0.03508771929824561), (1560940622978207744, 20, 0.07317073170731707), (1560941199627894784, 20, 0.022222222222222223), (1560943340576772104, 20, 0.023255813953488372), (1560947328785195008, 19, 0.02040816326530612), (1560947861780631552, 20, 0.0392156862745098), (1560948956317761536, 20, 0.020833333333333332), (1560949672700780544, 20, 0.0196078431372549), (1560949796587855872, 20, 0.019230769230769232), (1560950120656543744, 20, 0.023809523809523808), (1560951452524285952, 20, 0.023255813953488372), (1560952304576221184, 20, 0.018867924528301886), (1560952492782911488, 20, 0.02), (1560955653559963648, 20, 0.023255813953488372), (1560957931071455232, 20, 0.019230769230769232), (1560957936645578752, 20, 0.023809523809523808), (1560959983658979328, 20, 0.08333333333333333), (1560960036624646144, 20, 0.08333333333333333), (1560963704568553472, 19, 0.025), (1560964867103887360, 20, 0.0425531914893617), (1560965633940004864, 20, 0.022222222222222223), (1560966377766256640, 20, 0.019230769230769232), (1560968136828010496, 20, 0.0392156862745098), (1560968763838799872, 20, 0.05263157894736842), (1560970242960101376, 20, 0.0425531914893617), (1560974612321771528, 20, 0.022222222222222223), (1560980976448282624, 20, 0.03636363636363636), (1560981903603372032, 20, 0.03225806451612903), (1560982484484890624, 20, 0.029411764705882353), (1560983850033217536, 20, 0.021739130434782608), (1560983895503429632, 20, 0.025), (1560985458976653312, 20, 0.02), (1560986583884566528, 19, 0.045454545454545456), (1560990301354971144, 20, 0.017543859649122806), (1560990331989942272, 20, 0.017241379310344827), (1560991996856369152, 20, 0.02), (1560992252985483264, 20, 0.045454545454545456), (1560994691096649728, 19, 0.07142857142857142), (1560995401980956672, 20, 0.021739130434782608), (1560995529835913216, 20, 0.022727272727272728), (1560996670669615104, 20, 0.04), (1560996833408712704, 19, 0.021739130434782608), (1560998513131020288, 20, 0.02127659574468085), (1560999237533716480, 20, 0.023809523809523808), (1561001378969014272, 20, 0.025), (1561002173856628736, 20, 0.03773584905660377), (1561002935584100352, 20, 0.020833333333333332), (1561003007487221760, 20, 0.025), (1561004363052732416, 20, 0.04878048780487805), (1561005400811606016, 20, 0.017543859649122806), (1561007980505665536, 20, 0.020833333333333332), (1561008123455864832, 20, 0.041666666666666664), (1561008697119191040, 20, 0.04), (1561008985737773056, 20, 0.02127659574468085), (1561016015726579712, 20, 0.045454545454545456), (1561016983889518592, 20, 0.029411764705882353), (1561017621109248000, 20, 0.030303030303030304), (1561019207684440064, 20, 0.046511627906976744), (1561020497021771776, 20, 0.017241379310344827), (1561020805072535552, 20, 0.030303030303030304), (1561021809260220416, 20, 0.0425531914893617), (1561022303244255232, 20, 0.02), (1561026817607303168, 20, 0.022727272727272728), (1561026946066272256, 20, 0.021739130434782608), (1561027815767777280, 20, 0.05128205128205128), (1561028562488107008, 20, 0.047619047619047616), (1561028929040908288, 20, 0.020833333333333332), (1561029086767620096, 20, 0.041666666666666664), (1561031247891144704, 20, 0.043478260869565216), (1561031554675183616, 20, 0.022727272727272728), (1561032374162538496, 20, 0.043478260869565216), (1561033713403809792, 20, 0.0425531914893617), (1561036132640559104, 17, 0.0), (1561036298990850048, 20, 0.024390243902439025), (1561036592298414080, 20, 0.047619047619047616), (1561038268249956352, 20, 0.038461538461538464), (1561038709834792968, 20, 0.018518518518518517), (1561038885358129152, 20, 0.06382978723404255), (1561039705130823680, 20, 0.029411764705882353), (1561039808562167808, 20, 0.03636363636363636), (1561041696997113856, 20, 0.023255813953488372), (1561041726298521600, 20, 0.022222222222222223), (1561042723553132552, 20, 0.043478260869565216), (1561042757896097792, 20, 0.06976744186046512), (1561043339859968000, 20, 0.02040816326530612), (1561046128212250624, 20, 0.020833333333333332), (1561049054796304384, 20, 0.02), (1561050675831791616, 20, 0.08333333333333333), (1561051962615136256, 19, 0.023255813953488372), (1561052040956063744, 19, 0.023255813953488372), (1561055031205437440, 20, 0.05263157894736842), (1561057225325481984, 20, 0.044444444444444446), (1561057503978438656, 20, 0.02631578947368421), (1561065961419161600, 20, 0.027777777777777776), (1561067121148235776, 20, 0.0196078431372549), (1561067633893687296, 20, 0.020833333333333332), (1561068186514210816, 20, 0.021739130434782608), (1561068747519152128, 20, 0.02857142857142857), (1561070309955870720, 20, 0.01639344262295082), (1561072044963500032, 20, 0.02), (1561072925553426432, 19, 0.0), (1561074126588084224, 20, 0.014925373134328358), (1561075629692420096, 20, 0.020833333333333332), (1561077284303405056, 19, 0.02040816326530612), (1561077921300742144, 20, 0.02702702702702703), (1561078258942476288, 20, 0.023255813953488372), (1561079959497605120, 20, 0.022727272727272728), (1561080898300260352, 20, 0.03508771929824561), (1561081181361537024, 20, 0.022222222222222223), (1561081497779830784, 20, 0.06976744186046512), (1561082654543360000, 20, 0.021739130434782608), (1561083929616351232, 20, 0.07692307692307693), (1561084607327518720, 20, 0.0), (1561084674151030784, 20, 0.06818181818181818), (1561085010936995840, 20, 0.023255813953488372), (1561086188047712256, 20, 0.029411764705882353), (1561086489093480448, 20, 0.0392156862745098), (1561087437987876864, 20, 0.019230769230769232), (1561088584303595520, 19, 0.024390243902439025), (1561088726801039360, 20, 0.030303030303030304), (1561088752730230784, 20, 0.05128205128205128), (1561088877603229696, 20, 0.041666666666666664), (1561089253295136768, 20, 0.02564102564102564), (1561089601866895360, 20, 0.06521739130434782), (1561091966225379328, 14, 0.0), (1561094501128716288, 20, 0.019230769230769232), (1561096550578782208, 20, 0.022727272727272728), (1561098100202258432, 19, 0.02127659574468085), (1561098935996932096, 20, 0.047619047619047616), (1561099955561181184, 20, 0.038461538461538464), (1561102708362330112, 20, 0.06060606060606061), (1561104591575449600, 20, 0.04), (1561105128505020416, 20, 0.023255813953488372), (1561105700536950784, 20, 0.022222222222222223), (1561106614077296640, 20, 0.02127659574468085), (1561110209434632192, 20, 0.05), (1561113768523730944, 20, 0.018518518518518517), (1561115661799137280, 20, 0.0625), (1561115778656702464, 20, 0.019230769230769232), (1561117255257178112, 20, 0.0425531914893617), (1561130663016816640, 20, 0.044444444444444446), (1561131443438424064, 20, 0.02127659574468085), (1561135514530418688, 20, 0.02040816326530612), (1561135817787064320, 19, 0.020833333333333332), (1560924929930891264, 14, 0.0), (1560987487069143040, 18, 0.0), (1561027809673445376, 18, 0.018867924528301886), (1561100963070427136, 18, 0.022727272727272728), (1561128095377428480, 16, 0.0), (1560782048876548097, 20, 0.05), (1560795444242190337, 20, 0.022222222222222223), (1560800915883442177, 20, 0.04878048780487805), (1560807197910700033, 20, 0.025), (1560822986667704321, 20, 0.045454545454545456), (1560826212242653185, 20, 0.030303030303030304), (1560826536747474945, 20, 0.027777777777777776), (1560827382273134593, 20, 0.06060606060606061), (1560830961604087809, 20, 0.021739130434782608), (1560832499311312897, 20, 0.05555555555555555), (1560836179888549889, 20, 0.03333333333333333), (1560841965658333185, 19, 0.0), (1560842690572554241, 20, 0.037037037037037035), (1560846719247863809, 20, 0.01818181818181818), (1560852132898566145, 20, 0.020833333333333332), (1560853288672669697, 20, 0.018867924528301886), (1560854018058063873, 20, 0.043478260869565216), (1560855649055121409, 20, 0.05405405405405406), (1560859138376306689, 20, 0.019230769230769232), (1560860085676556289, 20, 0.020833333333333332), (1560861693135167489, 20, 0.02564102564102564), (1560873011443539969, 20, 0.025), (1560874315381448705, 20, 0.025), (1560877008594272257, 20, 0.0425531914893617), (1560878650647625729, 20, 0.038461538461538464), (1560886941176020993, 20, 0.02702702702702703), (1560888330895736833, 18, 0.0), (1560888770639060993, 20, 0.02), (1560889219077193729, 20, 0.02564102564102564), (1560889549106077697, 20, 0.05), (1560890043480395777, 20, 0.0392156862745098), (1560895327896485889, 20, 0.061224489795918366), (1560899509604990977, 20, 0.05405405405405406), (1560900410642239489, 20, 0.023255813953488372), (1560903892489183233, 20, 0.02127659574468085), (1560905630667399169, 18, 0.0), (1560907786434723841, 20, 0.025), (1560909568678531073, 20, 0.022727272727272728), (1560909707929436161, 20, 0.04878048780487805), (1560909803056136193, 20, 0.02127659574468085), (1560909923495583745, 20, 0.05454545454545454), (1560910783680319497, 20, 0.02040816326530612), (1560914708147970049, 20, 0.08333333333333333), (1560914970887454721, 20, 0.08571428571428572), (1560915390041047041, 20, 0.08333333333333333), (1560915750638047233, 20, 0.08571428571428572), (1560916964179992577, 20, 0.020833333333333332), (1560917715304472577, 20, 0.029411764705882353), (1560918609140105217, 20, 0.047619047619047616), (1560920659349458945, 20, 0.034482758620689655), (1560922405522100225, 20, 0.061224489795918366), (1560923476214829057, 20, 0.027777777777777776), (1560924588267159553, 20, 0.05263157894736842), (1560925610037444609, 19, 0.025), (1560927697194389505, 20, 0.02127659574468085), (1560931543069380609, 20, 0.027777777777777776), (1560931643250118657, 20, 0.037037037037037035), (1560932823690780673, 20, 0.018518518518518517), (1560933318199332865, 20, 0.04081632653061224), (1560934843520802817, 20, 0.021739130434782608), (1560938437448015873, 20, 0.02631578947368421), (1560938473556688897, 20, 0.020833333333333332), (1560938656483115009, 20, 0.02702702702702703), (1560943884796137473, 20, 0.02564102564102564), (1560944190598553601, 20, 0.044444444444444446), (1560946968834326529, 20, 0.025), (1560948494600286209, 20, 0.019230769230769232), (1560952422855483393, 20, 0.046511627906976744), (1560952530871484417, 20, 0.021739130434782608), (1560957232816263169, 20, 0.022222222222222223), (1560959174745509889, 20, 0.0), (1560963174198820865, 20, 0.03076923076923077), (1560963881404620801, 20, 0.03508771929824561), (1560964241569599489, 20, 0.03125), (1560968280449228801, 20, 0.047619047619047616), (1560971083272044545, 20, 0.04081632653061224), (1560975201801801729, 20, 0.017857142857142856), (1560975218331500545, 20, 0.018518518518518517), (1560979001593716737, 20, 0.024390243902439025), (1560981324457881601, 20, 0.024390243902439025), (1560981493597573121, 20, 0.04878048780487805), (1560982192783769601, 20, 0.044444444444444446), (1560982224329216001, 20, 0.030303030303030304), (1560982852698775553, 20, 0.046511627906976744), (1560994890514944001, 20, 0.03571428571428571), (1560995108517969921, 20, 0.0196078431372549), (1560995115325423617, 20, 0.04), (1560997512378556417, 20, 0.04), (1560999671811694593, 20, 0.0392156862745098), (1561001996269850625, 20, 0.018867924528301886), (1561003902148849665, 20, 0.02564102564102564), (1561004182987067393, 20, 0.018518518518518517), (1561004184161456129, 20, 0.04081632653061224), (1561004720222789633, 20, 0.04878048780487805), (1561005235878993921, 20, 0.08823529411764706), (1561007331990802433, 20, 0.0392156862745098), (1561008260131454977, 20, 0.02), (1561011143107289089, 20, 0.023255813953488372), (1561020095085920257, 20, 0.01818181818181818), (1561027587194888193, 20, 0.019230769230769232), (1561031685449297921, 20, 0.0425531914893617), (1561034410853638153, 20, 0.020833333333333332), (1561035597233389569, 20, 0.017241379310344827), (1561038028734357505, 20, 0.023809523809523808), (1561041322751967233, 20, 0.0425531914893617), (1561041418671587329, 20, 0.045454545454545456), (1561041859828490241, 20, 0.02), (1561043665971208193, 20, 0.024390243902439025), (1561044696432001025, 20, 0.0196078431372549), (1561046849175183361, 20, 0.038461538461538464), (1561047028137967617, 20, 0.02127659574468085), (1561047075504234497, 20, 0.023255813953488372), (1561049465707905025, 20, 0.022727272727272728), (1561050180291645441, 20, 0.02702702702702703), (1561050725303697409, 20, 0.08333333333333333), (1561051768905400321, 20, 0.08571428571428572), (1561053812009336833, 20, 0.022727272727272728), (1561055194124787713, 20, 0.021739130434782608), (1561055454398402561, 20, 0.037037037037037035), (1561056346623057921, 20, 0.03125), (1561067882498465793, 19, 0.02564102564102564), (1561069310461808641, 20, 0.022727272727272728), (1561070091005034497, 20, 0.045454545454545456), (1561080704200593409, 20, 0.08333333333333333), (1561082854976720897, 20, 0.023809523809523808), (1561092461119868929, 20, 0.02040816326530612), (1561093429119041537, 20, 0.041666666666666664), (1561095825249472513, 20, 0.046511627906976744), (1561096763846856705, 20, 0.02702702702702703), (1561100995039580161, 20, 0.03773584905660377), (1561102907151368193, 20, 0.03571428571428571), (1561109740528205825, 20, 0.0), (1561116594369175553, 20, 0.02127659574468085), (1561119530243563521, 20, 0.024390243902439025), (1561121685792665601, 20, 0.020833333333333332), (1561122583067639809, 20, 0.021739130434782608), (1561123809083723777, 20, 0.017543859649122806), (1561126947962458113, 20, 0.030303030303030304), (1561130750497509377, 20, 0.024390243902439025), (1561136380784234497, 20, 0.02040816326530612), (1561102185135452161, 18, 0.017543859649122806), (1560798772644515841, 18, 0.022222222222222223), (1560818906545233921, 18, 0.019230769230769232), (1560899436124876801, 18, 0.02631578947368421), (1560804864044814338, 20, 0.046511627906976744), (1560812965112303618, 16, 0.0), (1560827739338424322, 20, 0.023255813953488372), (1560836768685064194, 20, 0.02702702702702703), (1560873381330767874, 20, 0.018867924528301886), (1560875964636319746, 20, 0.0392156862745098), (1560876221805645826, 20, 0.01694915254237288), (1560886797143523330, 20, 0.02702702702702703), (1560901824688934914, 20, 0.038461538461538464), (1560910085831942146, 20, 0.030303030303030304), (1560914953820811266, 20, 0.024390243902439025), (1560915183148716034, 20, 0.08571428571428572), (1560915442511790082, 20, 0.08333333333333333), (1560921784244994050, 20, 0.022727272727272728), (1560924602536181762, 20, 0.0425531914893617), (1560925168863776770, 20, 0.018518518518518517), (1560943181524570114, 20, 0.061224489795918366), (1560943266018824194, 20, 0.027777777777777776), (1560945864251695106, 20, 0.058823529411764705), (1560948949472755714, 20, 0.02040816326530612), (1560961206801817602, 20, 0.08571428571428572), (1560978708055216130, 20, 0.04081632653061224), (1560980377388433410, 20, 0.04081632653061224), (1560982491476918274, 20, 0.022222222222222223), (1560984177574649858, 20, 0.027777777777777776), (1560993606223159298, 20, 0.021739130434782608), (1560995807121346562, 19, 0.0), (1560995929783615490, 20, 0.020833333333333332), (1560999827324153858, 20, 0.043478260869565216), (1561005655904976898, 20, 0.02), (1561006121216839682, 20, 0.022727272727272728), (1561006823242694658, 20, 0.07142857142857142), (1561013518006190082, 20, 0.04081632653061224), (1561016952902098946, 20, 0.05), (1561024369836658690, 20, 0.030303030303030304), (1561035461841207298, 20, 0.0425531914893617), (1561036188135440386, 20, 0.06382978723404255), (1561045240609472514, 20, 0.02564102564102564), (1561047107334717442, 20, 0.022727272727272728), (1561050697365442562, 20, 0.017543859649122806), (1561062054567596034, 20, 0.04878048780487805), (1561066585988767746, 17, 0.0), (1561071048401068034, 20, 0.025), (1561079711282991106, 20, 0.02702702702702703), (1561086398031187970, 20, 0.012658227848101266), (1561094551170850818, 20, 0.046511627906976744), (1561095981432803330, 20, 0.08333333333333333), (1561096346794344450, 20, 0.08333333333333333), (1561109131943198722, 20, 0.07142857142857142), (1561126444587274242, 20, 0.024390243902439025), (1561139735774593026, 20, 0.034482758620689655), (1560996106959568898, 18, 0.0196078431372549), (1560857662174142467, 20, 0.02702702702702703), (1560861221770838019, 16, 0.0), (1560866772672184323, 20, 0.02702702702702703), (1560870269522255875, 20, 0.02127659574468085), (1560895791748812803, 20, 0.022222222222222223), (1560906727813517315, 19, 0.0), (1560909934245683203, 20, 0.03773584905660377), (1560945334385365027, 20, 0.038461538461538464), (1560946361079570435, 20, 0.06), (1560950312978071555, 20, 0.044444444444444446), (1560955025110847491, 20, 0.020833333333333332), (1560955193948344323, 20, 0.047619047619047616), (1560958746641289219, 20, 0.038461538461538464), (1560972963721236483, 20, 0.04878048780487805), (1560985022366187523, 20, 0.024390243902439025), (1560986375142391811, 20, 0.0425531914893617), (1560988438505078787, 20, 0.02631578947368421), (1560989561555124227, 20, 0.02127659574468085), (1560993067175673859, 20, 0.022727272727272728), (1560996551798886403, 20, 0.020833333333333332), (1561017026579136515, 20, 0.05128205128205128), (1561027561022423043, 20, 1.0), (1561028090888916995, 20, 0.02631578947368421), (1561031928391864323, 20, 0.030303030303030304), (1561037027235225603, 20, 0.07407407407407407), (1561037465724637187, 20, 0.030303030303030304), (1561038484328136707, 20, 0.022727272727272728), (1561042760534310915, 20, 0.041666666666666664), (1561043609616584707, 20, 0.024390243902439025), (1561067936709808131, 20, 0.04081632653061224), (1561071081754181635, 20, 0.03636363636363636), (1561071925639651331, 20, 0.05), (1561079138630393859, 20, 0.045454545454545456), (1561079569288921091, 19, 0.05128205128205128), (1561091870243098627, 20, 0.021739130434782608), (1561095998222602243, 20, 0.017857142857142856), (1561101581025779715, 20, 0.024390243902439025), (1561112127984877571, 20, 0.06382978723404255), (1561118971411275779, 20, 0.022727272727272728), (1561123958715518979, 20, 0.017543859649122806), (1561127074173181955, 20, 0.029411764705882353), (1560969062427009027, 18, 0.0), (1560790230986268676, 20, 0.02564102564102564), (1560914760765476868, 20, 0.08333333333333333), (1560959118155849732, 20, 0.016666666666666666), (1561012070782484484, 20, 0.027777777777777776), (1561012076038029316, 20, 0.0196078431372549), (1561015593087733764, 20, 0.01818181818181818), (1561075369557598212, 20, 0.0), (1561078295961210884, 20, 0.024390243902439025), (1561135306526670852, 20, 0.0196078431372549), (1560928553373425668, 19, 0.025), (1560786453969395717, 20, 0.029411764705882353), (1560837913713053701, 20, 0.037037037037037035), (1560842371993985029, 20, 0.045454545454545456), (1560852780650303493, 20, 0.021739130434782608), (1560877542856429573, 20, 0.047619047619047616), (1560944750722785285, 20, 0.0), (1560987266796896261, 20, 0.019230769230769232), (1560987875159711749, 20, 0.0625), (1560998442805170181, 20, 0.05555555555555555), (1561005693611786245, 20, 0.04918032786885246), (1561009740540481541, 20, 0.044444444444444446), (1561015875762896901, 20, 0.04081632653061224), (1561032678828285957, 20, 0.034482758620689655), (1561049166100328453, 20, 0.0196078431372549), (1561064153439121413, 20, 0.045454545454545456), (1561076219851988997, 20, 0.024390243902439025), (1561111419088674821, 20, 0.019230769230769232), (1561112671117869061, 20, 0.045454545454545456), (1560948454121242630, 20, 0.02702702702702703), (1560955670379130886, 20, 0.05714285714285714), (1560959885688422406, 20, 0.04081632653061224), (1561044034776223750, 20, 0.041666666666666664), (1561056926863167494, 20, 0.04), (1561065610943123462, 20, 0.02040816326530612), (1561086705637228550, 20, 0.041666666666666664), (1561097824942657542, 20, 0.027777777777777776), (1561121391402958854, 20, 0.030303030303030304), (1560969358473678855, 20, 0.05128205128205128), (1561013961952305167, 20, 0.075), (1561032659769466887, 20, 0.0425531914893617), (1561056011691098119, 20, 0.02), (1561096640005537799, 20, 0.018518518518518517), (1561139274434678791, 20, 0.06382978723404255)]\n",
            "=========================\n",
            "20 1000 3 100\n",
            "-------------------------\n",
            "[(1561027561022423043, 20, 1.0), (1560917794593673221, 3, 0.03125)]\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "search_record = en_small_twits_df[en_small_twits_df['tweetid'] == 1561027561022423043].iloc[0][['text','tweetid']]\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 100 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 100 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 200 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 200')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 10 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('10 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 25 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('25 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 1 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 1 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 3 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 3 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECDCiWWqDWxk"
      },
      "source": [
        "1 - only target tweet + 1 similar text tweet (sim = 16/20, differences: emojie, new lines, missing full hashtags or missing hashtag sighn ('#'), some punctuation and/or space (space between words) errors)\n",
        "\n",
        "2 - same as (1) + false possitive results (sim = 3/4)\n",
        "\n",
        "3 - same as (1), but second tweet have sim = 10/20 (in (1) 16/20)\n",
        "\n",
        "4 - same as (1), but second tweet have sim = 6/10 (~12/20) (in (1) 16/20)\n",
        "\n",
        "5 - same as (1), but second tweet have sim = 21/25 (~16.8/20) (in (1) 16/20)\n",
        "\n",
        "6 - Have lots of FP with close to max sim (both tweets with sim = 20 are same as in (1)). total results 49\n",
        "\n",
        "7 - same as (1), but second tweet have sim = 15/20 (in (1) 16/20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "xTYdppFbDWxl",
        "outputId": "404f3a6e-f755-42b0-f0aa-266c795f4792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 1000 2 100\n",
            "-------------------------\n",
            "[(1561076241851097091, 20, 1.0), (1561077604270186501, 16, 0.9523809523809523)]\n",
            "=========================\n",
            "20 100 2 100\n",
            "-------------------------\n",
            "[(1560801662163652608, 3, 0.02631578947368421), (1561058667121836032, 3, 0.037037037037037035), (1561123987845181440, 3, 0.02702702702702703), (1560829403340218368, 3, 0.0), (1560882556211445760, 3, 0.06666666666666667), (1561091589765619712, 3, 0.0), (1561108428419989504, 3, 0.06666666666666667), (1560974933819154432, 3, 0.034482758620689655), (1561052599469629440, 4, 0.03125), (1560786428371648512, 3, 0.03125), (1561042462193303552, 3, 0.0), (1560792608493953024, 3, 0.02564102564102564), (1560848110896701440, 3, 0.05128205128205128), (1560853232515026944, 3, 0.05128205128205128), (1560873414750978048, 3, 0.05128205128205128), (1560921085205483520, 3, 0.04878048780487805), (1560912032785989633, 3, 0.04), (1560874526233296897, 3, 0.0), (1561114850838958081, 3, 0.0), (1560910001522221058, 3, 0.0), (1560935410167152642, 3, 0.0), (1561076241851097091, 20, 1.0), (1561140735621189636, 3, 0.04878048780487805), (1561077604270186501, 16, 0.9523809523809523), (1560835806876639239, 3, 0.02857142857142857)]\n",
            "=========================\n",
            "20 1000 2 200\n",
            "-------------------------\n",
            "[(1561076241851097091, 20, 1.0), (1561077604270186501, 10, 0.9523809523809523)]\n",
            "=========================\n",
            "10 1000 2 100\n",
            "-------------------------\n",
            "[(1561076241851097091, 10, 1.0), (1561077604270186501, 6, 0.9523809523809523)]\n",
            "=========================\n",
            "25 1000 2 100\n",
            "-------------------------\n",
            "[(1561076241851097091, 25, 1.0), (1561077604270186501, 21, 0.9523809523809523)]\n",
            "=========================\n",
            "20 1000 1 100\n",
            "-------------------------\n",
            "[(1560782605116755968, 16, 0.1), (1560790944529645568, 17, 0.05405405405405406), (1560838796773470208, 14, 0.03333333333333333), (1560851227377688576, 16, 0.05), (1560883405084049408, 14, 0.025), (1560969052264099840, 15, 0.045454545454545456), (1560971902029643776, 15, 0.07692307692307693), (1561007859663601664, 15, 0.06976744186046512), (1561036606521233416, 18, 0.03125), (1561081833110048768, 17, 0.07894736842105263), (1561082472598646784, 17, 0.10344827586206896), (1560786544687869952, 14, 0.07317073170731707), (1560891388744634368, 16, 0.04081632653061224), (1560930242256068608, 15, 0.05), (1560948860201082880, 11, 0.03225806451612903), (1561011219154231296, 11, 0.03225806451612903), (1561040824124719104, 16, 0.10344827586206896), (1560921378488041472, 9, 0.02631578947368421), (1561028411551846400, 10, 0.02631578947368421), (1561001687522885633, 18, 0.041666666666666664), (1561017731197145089, 13, 0.075), (1561021958363512833, 17, 0.08), (1561026178177290241, 15, 0.029411764705882353), (1561120144402915329, 15, 0.04878048780487805), (1561124477072801793, 18, 0.07407407407407407), (1561128002792161281, 17, 0.024390243902439025), (1561140089240952833, 18, 0.06451612903225806), (1560849300753485825, 15, 0.024390243902439025), (1560890064833593345, 11, 0.03333333333333333), (1561035207691616257, 14, 0.0), (1561044945456209921, 15, 0.02631578947368421), (1560799862869852162, 18, 0.08333333333333333), (1560988783977406466, 14, 0.047619047619047616), (1561020626453905410, 13, 0.05714285714285714), (1561034204829335554, 15, 0.04878048780487805), (1560804969195913218, 17, 0.07407407407407407), (1560955313326620674, 16, 0.03225806451612903), (1560871414713819139, 17, 0.06060606060606061), (1561076241851097091, 20, 1.0), (1561079976794923011, 17, 0.08), (1561111038468161539, 18, 0.08823529411764706), (1561043446172893187, 10, 0.02702702702702703), (1560899471764066308, 14, 0.0425531914893617), (1561033337346691077, 15, 0.046511627906976744), (1561077604270186501, 20, 0.9523809523809523), (1560936018173460486, 16, 0.1724137931034483), (1561131154132140038, 15, 0.044444444444444446), (1560930983376977927, 17, 0.034482758620689655), (1561061319767429127, 17, 0.06060606060606061)]\n",
            "=========================\n",
            "20 1000 3 100\n",
            "-------------------------\n",
            "[(1561076241851097091, 20, 1.0), (1561077604270186501, 15, 0.9523809523809523)]\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "search_record = en_small_twits_df[en_small_twits_df['tweetid'] == 1561076241851097091].iloc[0][['text','tweetid']]\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 100 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 100 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 200 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 200')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 10 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('10 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 25 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('25 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 1 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 1 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 3 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 3 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYhvlVDHDWxl"
      },
      "source": [
        "1 - only target tweet + 1 similar text tweet (sim = 8/20, differences: @user)\n",
        "\n",
        "2 - same as (1) + false possitive results (sim = 3/4)\n",
        "\n",
        "3 - only target tweet\n",
        "\n",
        "4 - only target tweet\n",
        "\n",
        "5 - same as (1), but second tweet have sim = 10/25 (~8/20) (in (1) 8/20)\n",
        "\n",
        "6 - same as (1), but second tweet have sim = 6/20 (in (1) 8/20)\n",
        "\n",
        "7 - same as (1), but second tweet have sim = 4/20 (in (1) 8/20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "jzSmjl_LDWxl",
        "outputId": "453a1d09-4504-4ea0-85c8-7e486dab9671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 1000 2 100\n",
            "-------------------------\n",
            "[(1560784633637265409, 8, 0.7666666666666667), (1560790174740746243, 20, 1.0)]\n",
            "=========================\n",
            "20 100 2 100\n",
            "-------------------------\n",
            "[(1561089290863448064, 3, 0.0), (1560950749202358272, 4, 0.0), (1560925074294800384, 3, 0.04878048780487805), (1560784951984812032, 3, 0.0), (1560958839314386944, 3, 0.0), (1561032105760624640, 3, 0.0), (1560922239851184128, 3, 0.0), (1561067847392104449, 3, 0.0), (1560784633637265409, 8, 0.7666666666666667), (1560911435240296449, 3, 0.0), (1560857547594014721, 3, 0.0), (1561031233328627713, 3, 0.022727272727272728), (1561066820777390081, 3, 0.0), (1561008746201063425, 4, 0.023809523809523808), (1560794310328881154, 4, 0.0), (1560874145956503554, 3, 0.0), (1560853804303765506, 3, 0.0), (1560790174740746243, 20, 1.0), (1561043384986476547, 3, 0.0), (1561061319117361155, 3, 0.0), (1561065534531182597, 3, 0.0)]\n",
            "=========================\n",
            "20 1000 2 200\n",
            "-------------------------\n",
            "[(1560790174740746243, 20, 1.0)]\n",
            "=========================\n",
            "10 1000 2 100\n",
            "-------------------------\n",
            "[(1560790174740746243, 10, 1.0)]\n",
            "=========================\n",
            "25 1000 2 100\n",
            "-------------------------\n",
            "[(1560784633637265409, 10, 0.7666666666666667), (1560790174740746243, 25, 1.0)]\n",
            "=========================\n",
            "20 1000 1 100\n",
            "-------------------------\n",
            "[(1560784633637265409, 6, 0.7666666666666667), (1560790174740746243, 20, 1.0)]\n",
            "=========================\n",
            "20 1000 3 100\n",
            "-------------------------\n",
            "[(1560784633637265409, 4, 0.7666666666666667), (1560790174740746243, 20, 1.0)]\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "search_record = en_small_twits_df[en_small_twits_df['tweetid'] == 1560790174740746243].iloc[0][['text','tweetid']]\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 100 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 100 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 200 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 2 200')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 10 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('10 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 25 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 2 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('25 1000 2 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 1 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 1 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')\n",
        "\n",
        "BANDS = 20 # num of bands\n",
        "BUCKETS = 1000 # num of buckets in each band (total = k*b)\n",
        "TOKENS = 3 # num of shingles in k-gramm\n",
        "N_HASH = 100 # num of hash functions\n",
        "MAX_HASH = 2**32 # max value of hash\n",
        "primes = generate_primes(N_HASH, MAX_HASH)\n",
        "\n",
        "shingles = rdd.map(lambda s: (s[0], n_gram(tokenize(s[1]),TOKENS)))\n",
        "lsh_s = shingles.map(lambda x: (x[0],signature(x[1], N_HASH, MAX_HASH, primes))).map(lambda x: (x[0], lsh(x[1], BANDS, BUCKETS))).map(lambda x: [(s, x[0]) for s in x[1]]).flatMap(lambda x: x).groupByKey().mapValues(list)\n",
        "\n",
        "search_signature = (search_record['tweetid'],lsh(signature(n_gram(tokenize(search_record['text']),TOKENS),N_HASH,MAX_HASH, primes), BANDS, BUCKETS))\n",
        "answer = lsh_s.filter(lambda x: x[0] in search_signature[1]).flatMap(lambda x: x[1]).map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).filter(lambda x: x[1]>=3).map(\n",
        "    lambda x: (x[0],x[1],jaccard(tokenize(search_record['text']),tokenize(en_small_twits_df[en_small_twits_df['tweetid'] == x[0]].iloc[0]['text'])))).collect()\n",
        "print('20 1000 3 100')\n",
        "print('-------------------------')\n",
        "print(answer)\n",
        "print('=========================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQONFlCsDWxm"
      },
      "source": [
        "To read text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "JZLY1uN5DWxm",
        "outputId": "1b7d0b48-a553-45e8-94c6-67f81c1fe41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Russia put\"on pause\"the holding of\"referendums\"\n",
            "Most likely,the invaders did not expect the decisiveness of the Armed Forces of #Ukraine️. In Russia,they were shocked by the explosions in the #Crimea, they didn’t think that such a thing would ever happen.\n",
            "#russiaisateroriststate https://t.co/kpt1JvSK7K\n"
          ]
        }
      ],
      "source": [
        "print(en_small_twits_df[en_small_twits_df['tweetid'] == 1561076241851097091].iloc[0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzU4uz3WDWxm",
        "outputId": "7cb47ce8-05cd-4557-e5dd-18f276aa86de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔎 #Russia put \"on pause\" the holding of \"#referendums\"\n",
            "\n",
            "Most likely, the invaders did not expect the decisiveness of the Armed Forces of #Ukraine️. In Russia, they were shocked by the explosions in the #Crimea, they didn’t think that such a thing would ever happen https://t.co/4Hut7x2zIB\n"
          ]
        }
      ],
      "source": [
        "print(en_small_twits_df[en_small_twits_df['tweetid'] == 1561077604270186501].iloc[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng3y9JllDWxm"
      },
      "source": [
        "To read text from list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "oVSK41HADWxm",
        "outputId": "a9ecdbd5-c678-4859-e747-17aaa2d72193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Very observant of him. It's been obvious to me for a long time too. #Putin #vladimir #Russians #USA #bureaucracy #uniparty #Elites #EliteCoup https://t.co/fqxeLpX9N4\n",
            "3\n",
            "==========================\n",
            "Ring walk 22.30, it's been broadcast in #Ukraine for free, \n",
            "\n",
            "I think that #AnthonyJoshua is in for a tough time! \n",
            "\n",
            "Links to streaming in the Youtube clip attached.. \n",
            "#UsykJoshua2\n",
            "\n",
            "https://t.co/n0BQz3iOhD\n",
            "4\n",
            "==========================\n",
            "@martinplaut For half a century this is what you can give them, deteriorating life.\n",
            "#TPLFisTheCause 4suffer, #war hunger, displacement, death..on #Tigray ppl for half a century.\n",
            "#NoMore\n",
            "3\n",
            "==========================\n",
            "Decorative pear Felt handmade https://t.co/KL0QycVakL via @EtsySocial  #giftideas #Ukraine #cushionforneedles #seamstressgift https://t.co/rEBJKIoxhf\n",
            "3\n",
            "==========================\n",
            "I stand with #Ukraine! https://t.co/2ExYNzS67s\n",
            "3\n",
            "==========================\n",
            "Quotes that no one should say/believe:\n",
            "\n",
            "\"I trust #Trump completely! #GOP forever!\"\n",
            "\n",
            "\"I trust #Biden completely! #Dems forever!\"\n",
            "\n",
            "#BeCritcal #BeFair #BeResponsible #BeIndependent #TrustYourself #UnityIsPatriotic #TruthIsElusive\n",
            "3\n",
            "==========================\n",
            "Rare Cupid And Barrel Silve... https://t.co/LrzeLshnD3 via @topbananamall fab @topbananaantiques #dogsoftwitter #antiquejewllery #gardensoftwitter  #StandWithUkraine #antiquesilver #stopclimatechange #antiquepictures #celebritymasterchef\n",
            "3\n",
            "==========================\n",
            "@strategywoman Oh yes...do go along...would go too if I could...#SlavaUkraini   🇺🇦💛💙\n",
            "3\n",
            "==========================\n",
            "@Faiza_AminTV @DrTedros The head of the #WHO is also from the inner circle of #Ethiopia's dictatorial old guard, the ethnonationalist TPLF, which launched this war against a still-fragile multiethnic democracy.\n",
            "\n",
            "The only obstacle to getting food aid to #Tigray has been the #TPLF attacking other regions.\n",
            "8\n",
            "==========================\n",
            "The US Defence Department on Friday announced a new $775 million package of defence equipment and ammunition for Ukraine, including various types of missiles, artillery and mine-clearing systems.\n",
            "https://t.co/tmxXE5iNar\n",
            "\n",
            "#US  #armspackage  #Ukraine https://t.co/ijFnKoi1nz\n",
            "3\n",
            "==========================\n",
            "Pentagon reveals details of NASAMS supplies to Ukraine https://t.co/gtquDXZo3R  | UA no-fly zone #StopPutin #ArmUkraineNow #StandUpforUkraine\n",
            "3\n",
            "==========================\n",
            "@SamRamani2 Botulinum toxin? More likely that Russian soldiers stole preserved food from someone and caught a few bad jars. But #Ukraine is the culprit. Sure, Jane! https://t.co/Ix9bZelhgq\n",
            "3\n",
            "==========================\n",
            ".\n",
            ".\n",
            "\n",
            "     🇷🇺         🇺🇦\n",
            ".\n",
            "\n",
            " ⚡️ “Shell\" in #Sevastopol shoots down the enemy targets .\n",
            "\n",
            "　     Governor of Sevastopol said that Air defense is working .\n",
            "\n",
            ".\n",
            "                                                                THE #Kiev Forces\n",
            ".\n",
            ".\n",
            "   #Russia\n",
            ".\n",
            "https://t.co/wAXwPwITtH https://t.co/AJx1SBLr8n\n",
            "3\n",
            "==========================\n",
            "@laurenboebert Of course a traitor to our republic wouldn't want to defend #democracy in #Ukrainian against their dear leader's master! \n",
            "\n",
            "That would be too much like right!\n",
            "4\n",
            "==========================\n",
            "Breaking : The #Ukrainian armed forces are bombing #Melitopol in the #Za... https://t.co/fhtrqLrZeI via @YouTube\n",
            "4\n",
            "==========================\n",
            "If they cannot sell you vaccines and poisons?\n",
            "WARS.\n",
            "🇲🇾#Malaysia #Indonesia #Russia #Britain #London #UK  #Australia #Sidney #Melbourne #Germany #Berlin #Hamburg #Stuttgart #France #Spain #Portugal #Italy #Finland #Singapore\n",
            "https://t.co/YX8iwRKWB2\n",
            "3\n",
            "==========================\n",
            "Look at this clown.\n",
            "\n",
            "#UkraineRussiaWar #UkraineWar https://t.co/4unz3mk2RZ\n",
            "3\n",
            "==========================\n",
            "@PeterTatchell @SWTigray @OmnaTigray @tigrayyouthntwk @womenoftigray The head of the #WHO is also from the inner circle of #Ethiopia's dictatorial old guard, the ethnonationalist TPLF, which launched this war against a still-fragile multiethnic democracy.\n",
            "\n",
            "The only obstacle to getting food aid to #Tigray has been the #TPLF attacking other regions.\n",
            "20\n",
            "==========================\n",
            "We stand with #Ukraine https://t.co/Hzz0lnsiQ1\n",
            "3\n",
            "==========================\n",
            "@BasilLombardi @BaseerU91552443 @BBCWorld Go fight for your #Zelenskyy puppet regime, time is running out for you to gloriously fertilize the fields on his behalf. https://t.co/eOMHBcfPPO\n",
            "3\n",
            "==========================\n",
            "The #Ukrainian aircraft works 🔥\n",
            "\n",
            "#UkrainianArmy #UkraineRussiaWar https://t.co/249s70ToYy\n",
            "3\n",
            "==========================\n",
            "21\n"
          ]
        }
      ],
      "source": [
        "li = [(1561089290863448064, 3), (1560950749202358272, 4), (1560925074294800384, 3), (1560784951984812032, 3), (1560958839314386944, 3), (1561032105760624640, 3), (1560922239851184128, 3), (1561067847392104449, 3), (1560784633637265409, 8), (1560911435240296449, 3), (1560857547594014721, 3), (1561031233328627713, 3), (1561066820777390081, 3), (1561008746201063425, 4), (1560794310328881154, 4), (1560874145956503554, 3), (1560853804303765506, 3), (1560790174740746243, 20), (1561043384986476547, 3), (1561061319117361155, 3), (1561065534531182597, 3)]\n",
        "for l in li:\n",
        "    #if l[1] > 10:\n",
        "        #if l[0] == 1561001737292754945:\n",
        "        #    pass\n",
        "    print(en_small_twits_df[en_small_twits_df['tweetid'] == l[0]].iloc[0]['text'])\n",
        "    print(l[1])\n",
        "    print('==========================')\n",
        "print(len(li))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search of similar tweets from n targets (one by one):\n",
        "\n",
        "take sample from rdd and print a list of: \n",
        "\n",
        "(*targetID*, *list of similar tweetIDs*)"
      ],
      "metadata": {
        "id": "jsuyphyOBpe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = rdd.sample(False, 0.001).collect()\n",
        "similarity = 3\n",
        "result = []\n",
        "for sample in samples:\n",
        "    result.append(search_similar(sample[0], sample[1], lsh_s, similarity))\n",
        "print(result)"
      ],
      "metadata": {
        "id": "uErGlZVfBsuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same code, but as function:"
      ],
      "metadata": {
        "id": "BrzDSP7UBwDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_from_sample(rdd, fraction, similarity):\n",
        "    if fraction > 1 or fraction < 0:\n",
        "        print('Wrong fraction of sample')\n",
        "        print('Fraction have to be in range of (0,1)')\n",
        "        return None\n",
        "    samples = rdd.sample(False, fraction).collect()\n",
        "    result = []\n",
        "    for sample in samples:\n",
        "        result.append(search_similar(sample[0], sample[1], lsh_s, similarity))\n",
        "    return result"
      ],
      "metadata": {
        "id": "u8IfyP8iBu6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(search_from_sample(rdd, 0.001,3))"
      ],
      "metadata": {
        "id": "arfd_Xd2B02p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}